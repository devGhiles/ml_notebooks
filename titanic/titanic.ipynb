{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this kernel, I'm working on the Titanic kaggle challenge: https://www.kaggle.com/c/titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read all the data in the train file\n",
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of a table of 11 columns:\n",
    "* PassengerId: obvious\n",
    "* Survived: 0 -> no, 1 -> yes; the target variable\n",
    "* Pclass: 1 -> upper, 2 -> middle, 3 -> lower\n",
    "* Name: passenger name, probably irrelevant\n",
    "* Sex: male, female\n",
    "* Age: obvious\n",
    "* SibSp: # of siblings or spouses aboard\n",
    "* Parch: # of parent or children aboard\n",
    "* Ticket: ticket number (I doubt it will be relevant)\n",
    "* Fare: depends on the class I guess\n",
    "* Cabin: cabin number (I don't think it will be relvant. All the information it may bring is I think -a priori- carried by Pclass and eventually Fare)\n",
    "* Embarked: port of embarkation; C = Cherbourg, Q = Queenstown, S = Southampton (I would say it's irrelevant, we'll see if it is or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    891\n",
       "Survived       891\n",
       "Pclass         891\n",
       "Name           891\n",
       "Sex            891\n",
       "Age            714\n",
       "SibSp          891\n",
       "Parch          891\n",
       "Ticket         891\n",
       "Fare           891\n",
       "Cabin          204\n",
       "Embarked       889\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 891 rows. We see that the columns 'Age', 'Cabin' and 'Embarked' have missing values; we need to fill them. We'll also transform the categorical data into numerical (ordinal) data because it's easier to work with. Finally, we'll drop the 'PassengerId' and 'Name' columns because they don't provide any information necessary for the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # fill in the missing values\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['Cabin'].fillna('UNK', inplace=True)\n",
    "    df['Embarked'].fillna('UNK', inplace=True)\n",
    "    \n",
    "    # categorical data encoding\n",
    "    enc = LabelEncoder()\n",
    "    columns_to_encode = ['Sex', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "    for column in columns_to_encode:\n",
    "        df[column] = enc.fit_transform(df[column])\n",
    "    \n",
    "    # drop the 'PassengerId' and 'Name' columns\n",
    "    df.drop('PassengerId', axis=1, inplace=True)\n",
    "    df.drop('Name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived    891\n",
      "Pclass      891\n",
      "Sex         891\n",
      "Age         891\n",
      "SibSp       891\n",
      "Parch       891\n",
      "Ticket      891\n",
      "Fare        891\n",
      "Cabin       891\n",
      "Embarked    891\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>18</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>596</td>\n",
       "      <td>207</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>669</td>\n",
       "      <td>41</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>189</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>472</td>\n",
       "      <td>43</td>\n",
       "      <td>147</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age  SibSp  Parch  Ticket  Fare  Cabin  Embarked\n",
       "0         0       3    1  22.0      1      0     523    18    147         2\n",
       "1         1       1    0  38.0      1      0     596   207     81         0\n",
       "2         1       3    0  26.0      0      0     669    41    147         2\n",
       "3         1       1    0  35.0      1      0      49   189     55         2\n",
       "4         0       3    1  35.0      0      0     472    43    147         2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(data)\n",
    "print(data.count())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the columns have 891 values, and we see numbers, numbers everywhere, that's what we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>29.361582</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>338.528620</td>\n",
       "      <td>105.355780</td>\n",
       "      <td>130.744108</td>\n",
       "      <td>1.538721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>13.019697</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>200.850657</td>\n",
       "      <td>70.082521</td>\n",
       "      <td>36.024237</td>\n",
       "      <td>0.794231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>158.500000</td>\n",
       "      <td>40.500000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>337.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>519.500000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>680.000000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Survived      Pclass         Sex         Age       SibSp       Parch  \\\n",
       "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.383838    2.308642    0.647587   29.361582    0.523008    0.381594   \n",
       "std      0.486592    0.836071    0.477990   13.019697    1.102743    0.806057   \n",
       "min      0.000000    1.000000    0.000000    0.420000    0.000000    0.000000   \n",
       "25%      0.000000    2.000000    0.000000   22.000000    0.000000    0.000000   \n",
       "50%      0.000000    3.000000    1.000000   28.000000    0.000000    0.000000   \n",
       "75%      1.000000    3.000000    1.000000   35.000000    1.000000    0.000000   \n",
       "max      1.000000    3.000000    1.000000   80.000000    8.000000    6.000000   \n",
       "\n",
       "           Ticket        Fare       Cabin    Embarked  \n",
       "count  891.000000  891.000000  891.000000  891.000000  \n",
       "mean   338.528620  105.355780  130.744108    1.538721  \n",
       "std    200.850657   70.082521   36.024237    0.794231  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%    158.500000   40.500000  147.000000    1.000000  \n",
       "50%    337.000000   94.000000  147.000000    2.000000  \n",
       "75%    519.500000  157.000000  147.000000    2.000000  \n",
       "max    680.000000  247.000000  147.000000    3.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b538fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlqn/D8c7Dxw8+L4yAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b2982a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b28d8080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b2829898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b27c4b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b28d24a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b2a58be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCpKkyv8HR8EQlq0LMrgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b28fb828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/1aUy1P2y143AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b26128d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFB4URQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b26a7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# empirical distributions of the features and the target\n",
    "for column in data.columns:\n",
    "    plt.hist(data[column], weights=np.ones(len(data)) / len(data))\n",
    "    plt.title('{0} empirical distribution'.format(column))\n",
    "    plt.xlabel('{0}'.format(column))\n",
    "    plt.ylabel('Empirical PDF')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following remarks:\n",
    "* Around 60% of the passengers survived (and around 40% died). The classes are relatively well balanced, so it makes sense to use the accuracy as the evaluation metric.\n",
    "* The lower class (Pclass = 3) makes up more than half of the passenger. The upper and middle class are somewhat similarly represented.\n",
    "* Females make up around 65% of the passengers, and males around 35%.\n",
    "* The age is distributed following what seems to be a gaussian distribution, with it's mean around 30. We must keep in mind that a significant number of the age values were missing (around 20%) and were replaced by the remaining values's median.\n",
    "* The SibSp column follows a Poisson distribution, with the value 0 being highly represented, the value 1 a little bit, and the other ones from there very rarely.\n",
    "* The Parch column also follows a Poisson distribution.\n",
    "* Tickets follow an almost uniform distribution.\n",
    "* Fares also follow a uniform-like distribution, with the exception of some values which are more highly represented.\n",
    "* Cabin is a column which has a lot of missing values (around 77%). Those missing values corresond to the pick in the histogram. The other values seem to be distributed uniformly, although we have to rescale and have more data in order to asses that more confidently. In any case, this information is probably useless due to the number of missing values.\n",
    "* Embarked has very few missing values. There are 3 ports of embarkation, they represent around 10%, 20% and 70% of the passengers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96b26e1c50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEcCAYAAAAoSqjDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b2686080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# correlation\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix, we see that the 'Survived' variable is highly correlated with 'Sex', 'Pclass', and surprisingly 'Fare'. The correlation with 'Age' is not as high as I expected it, maybe because of the missing values that were replaced by the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f96b5347438>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEGCAYAAACHGfl5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96b5347710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data['Age'].loc[data['Survived'] == 1], color='g', label='Survived')\n",
    "plt.hist(data['Age'].loc[data['Survived'] == 0], color='r', label='Died')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 distribution almost overlap. I guess age really doesn't play a significant role, although one might notice that for ages < 10, the survival rate is significantly higher; but this category is rare relative to the general population so it won't provide much discriminative information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a random forest classifier with the scikit-learn default hyperparameters as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044692737430168"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('Survived', axis=1).values\n",
    "y = data['Survived'].values\n",
    "clf = RandomForestClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline is about 80%, which is very high for a baseline. But let's not forget that the dataset is relatively small, and the test set used here contains less than 200 entries. Results may vary if I use a larger dataset.\n",
    "\n",
    "Now I'll work on improving that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first prepare the data for classification (mostly by doing one-hot encoding). Then, we'll test a bunch of classical classification algorithm using (mostly) the default hyperparameters of scikit-learn. After that, we'll do some hyperparameter tuning using cross-validation on the best performing basic classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age, SibSp and Parch will remain as they are, their quantitative nature corresponds to what they mean in reality.\n",
    "\n",
    "One-hot encoding will be applied to Pclass, Sex and Embarked.\n",
    "\n",
    "Ticket, Fare and Cabin represent categorical data with numerous possible values. We're going to keep only the most common values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoders = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(df, test=False):\n",
    "    # Age, SibSp and Parch\n",
    "    X = np.concatenate((df['Age'].values.reshape(-1, 1),\n",
    "                       df['SibSp'].values.reshape(-1, 1),\n",
    "                       df['Parch'].values.reshape(-1, 1)),\n",
    "                       axis=1)\n",
    "    \n",
    "    # one-hot encoding of Pclass, Sex and Embarked\n",
    "    for column in ['Pclass', 'Sex', 'Embarked']:\n",
    "        if test:\n",
    "            values = np.array(encoders[column].transform(df[column].values.reshape(-1, 1)).todense())\n",
    "        else:\n",
    "            encoders[column] = OneHotEncoder(handle_unknown='ignore')\n",
    "            values = np.array(encoders[column].fit_transform(df[column].values.reshape(-1, 1)).todense())\n",
    "        X = np.concatenate((X, values), axis=1)\n",
    "    \n",
    "    # For columns Ticket, Fare and Cabin, we only keep the most common values\n",
    "    num_values_to_keep = {\n",
    "        'Ticket': 8,\n",
    "        'Fare': 15,\n",
    "        'Cabin': 4\n",
    "    }\n",
    "    for column in ['Ticket', 'Fare', 'Cabin']:\n",
    "        if not test:\n",
    "            counts = Counter(df[column])\n",
    "            most_common_counts = counts.most_common(num_values_to_keep[column])\n",
    "            values_to_keep = list(map(lambda x: x[0], most_common_counts))\n",
    "            encoders[column] = OneHotEncoder(handle_unknown='ignore')\n",
    "            encoders[column].fit(np.array(values_to_keep).reshape(-1, 1))\n",
    "        values = np.array(encoders[column].transform(df[column].values.reshape(-1, 1)).todense())\n",
    "        X = np.concatenate((X, values), axis=1)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = feature_extraction(data)\n",
    "y = data['Survived'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    'mnb': MultinomialNB(),\n",
    "    'gnb': GaussianNB(),\n",
    "    'svm1': SVC(kernel='linear'),\n",
    "    'svm2': SVC(kernel='rbf'),\n",
    "    'svm3': SVC(kernel='sigmoid'),\n",
    "    'mlp1': MLPClassifier(),\n",
    "    'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n",
    "    'ada': AdaBoostClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    'gbc': GradientBoostingClassifier(),\n",
    "    'lr': LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "accuracies = dict()\n",
    "for clf_name in clfs:\n",
    "    clf = clfs[clf_name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracies[clf_name] = clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada': 0.84916201117318435,\n",
       " 'dtc': 0.78212290502793291,\n",
       " 'gbc': 0.83798882681564246,\n",
       " 'gnb': 0.4972067039106145,\n",
       " 'lr': 0.86033519553072624,\n",
       " 'mlp1': 0.84916201117318435,\n",
       " 'mlp2': 0.82681564245810057,\n",
       " 'mnb': 0.83240223463687146,\n",
       " 'rfc': 0.82122905027932958,\n",
       " 'svm1': 0.81564245810055869,\n",
       " 'svm2': 0.82681564245810057,\n",
       " 'svm3': 0.53072625698324027}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good results seem to be given by the SVM with RBF kernel and by the 2 layers feedforward neural network. I'll perform hyperparameter optimization on these two models next. Other models also give good results (AdaBoost, Gradient Boosting, Multinomial Naive Bayes, ...) but for now, I'll just work on the SVM and the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the kernel is RBF\n",
    "parameters = {\n",
    "    'C': [1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1, 10, 100], 'gamma': [0.001, 0.01, 0.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033475</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.635241</td>\n",
       "      <td>0.638047</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 1, 'gamma': 0.001}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.641414</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.638047</td>\n",
       "      <td>0.643098</td>\n",
       "      <td>0.634680</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.810887</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 1, 'gamma': 0.01}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037128</td>\n",
       "      <td>0.012155</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 1, 'gamma': 0.1}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030576</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.814254</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 10, 'gamma': 0.001}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.006919</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031744</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.831089</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 10, 'gamma': 0.01}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.829966</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.002861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.040955</td>\n",
       "      <td>0.010127</td>\n",
       "      <td>0.776655</td>\n",
       "      <td>0.936027</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 10, 'gamma': 0.1}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>0.009912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.033775</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.817059</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'C': 100, 'gamma': 0.001}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.006299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.052345</td>\n",
       "      <td>0.008060</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.884400</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 100, 'gamma': 0.01}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.750842</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.872054</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.012010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.055551</td>\n",
       "      <td>0.009839</td>\n",
       "      <td>0.762065</td>\n",
       "      <td>0.969136</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 100, 'gamma': 0.1}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.966330</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>0.000301</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.006496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       0.033475         0.013957         0.635241          0.638047       1   \n",
       "1       0.031749         0.012482         0.797980          0.810887       1   \n",
       "2       0.037128         0.012155         0.811448          0.860269       1   \n",
       "3       0.030576         0.011172         0.810325          0.814254      10   \n",
       "4       0.031744         0.009088         0.821549          0.831089      10   \n",
       "5       0.040955         0.010127         0.776655          0.936027      10   \n",
       "6       0.033775         0.008432         0.817059          0.824916     100   \n",
       "7       0.052345         0.008060         0.794613          0.884400     100   \n",
       "8       0.055551         0.009839         0.762065          0.969136     100   \n",
       "\n",
       "  param_gamma                      params  rank_test_score  split0_test_score  \\\n",
       "0       0.001    {'C': 1, 'gamma': 0.001}                9           0.626263   \n",
       "1        0.01     {'C': 1, 'gamma': 0.01}                5           0.801347   \n",
       "2         0.1      {'C': 1, 'gamma': 0.1}                3           0.781145   \n",
       "3       0.001   {'C': 10, 'gamma': 0.001}                4           0.811448   \n",
       "4        0.01    {'C': 10, 'gamma': 0.01}                1           0.814815   \n",
       "5         0.1     {'C': 10, 'gamma': 0.1}                7           0.764310   \n",
       "6       0.001  {'C': 100, 'gamma': 0.001}                2           0.814815   \n",
       "7        0.01   {'C': 100, 'gamma': 0.01}                6           0.750842   \n",
       "8         0.1    {'C': 100, 'gamma': 0.1}                8           0.754209   \n",
       "\n",
       "   split0_train_score  split1_test_score  split1_train_score  \\\n",
       "0            0.641414           0.636364            0.638047   \n",
       "1            0.821549           0.801347            0.803030   \n",
       "2            0.882155           0.828283            0.850168   \n",
       "3            0.819865           0.818182            0.811448   \n",
       "4            0.835017           0.831650            0.828283   \n",
       "5            0.949495           0.787879            0.932660   \n",
       "6            0.833333           0.824916            0.823232   \n",
       "7            0.900673           0.818182            0.872054   \n",
       "8            0.978114           0.764310            0.966330   \n",
       "\n",
       "   split2_test_score  split2_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.643098            0.634680      0.000736        0.001252   \n",
       "1           0.791246            0.808081      0.000549        0.000211   \n",
       "2           0.824916            0.848485      0.001516        0.000275   \n",
       "3           0.801347            0.811448      0.001287        0.000277   \n",
       "4           0.818182            0.829966      0.002840        0.000229   \n",
       "5           0.777778            0.925926      0.001669        0.000481   \n",
       "6           0.811448            0.818182      0.003052        0.000161   \n",
       "7           0.814815            0.880471      0.007843        0.000319   \n",
       "8           0.767677            0.962963      0.006651        0.000301   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.006919         0.002749  \n",
       "1        0.004762         0.007816  \n",
       "2        0.021472         0.015491  \n",
       "3        0.006919         0.003968  \n",
       "4        0.007274         0.002861  \n",
       "5        0.009655         0.009912  \n",
       "6        0.005723         0.006299  \n",
       "7        0.030981         0.012010  \n",
       "8        0.005723         0.006496  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score (for the test) is $0.82$ and it is obtained for $C = 10$ and $\\gamma = 0.01$. This is a little bit disappointing knowing that we got the same score in our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network hyperparameters optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first time, we'll tune the neural network's number of layers and number of units per layer. After that, we'll optimize other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'hidden_layer_sizes': [\n",
    "        [50,],\n",
    "        [100,],\n",
    "        [200,],\n",
    "        [50, 50],\n",
    "        [50, 100],\n",
    "        [100, 50],\n",
    "        [100, 100],\n",
    "        [200, 200],\n",
    "        [100, 100, 100]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghiles/anaconda3/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'hidden_layer_sizes': [[50], [100], [200], [50, 50], [50, 100], [100, 50], [100, 100], [200, 200], [100, 100, 100]]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier()\n",
    "clf = GridSearchCV(mlp, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301975</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.700337</td>\n",
       "      <td>0.728956</td>\n",
       "      <td>[50]</td>\n",
       "      <td>{'hidden_layer_sizes': [50]}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.705387</td>\n",
       "      <td>0.344140</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.085842</td>\n",
       "      <td>0.103075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.673058</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.854097</td>\n",
       "      <td>[100]</td>\n",
       "      <td>{'hidden_layer_sizes': [100]}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.872054</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.115879</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.012698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.753086</td>\n",
       "      <td>0.780022</td>\n",
       "      <td>[200]</td>\n",
       "      <td>{'hidden_layer_sizes': [200]}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.617845</td>\n",
       "      <td>0.510256</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.094450</td>\n",
       "      <td>0.114843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487056</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.756453</td>\n",
       "      <td>0.768238</td>\n",
       "      <td>[50, 50]</td>\n",
       "      <td>{'hidden_layer_sizes': [50, 50]}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.622896</td>\n",
       "      <td>0.627946</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.826599</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.341058</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.094689</td>\n",
       "      <td>0.099667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.537638</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.755331</td>\n",
       "      <td>0.775533</td>\n",
       "      <td>[50, 100]</td>\n",
       "      <td>{'hidden_layer_sizes': [50, 100]}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.829966</td>\n",
       "      <td>0.639731</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.377539</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.082030</td>\n",
       "      <td>0.099182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.416390</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.835578</td>\n",
       "      <td>[100, 50]</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 50]}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.816498</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.054002</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.013769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.678461</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.837262</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100]}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.256673</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.012010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.277355</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.815937</td>\n",
       "      <td>0.846240</td>\n",
       "      <td>[200, 200]</td>\n",
       "      <td>{'hidden_layer_sizes': [200, 200]}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.477167</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.010317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.581189</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>[100, 100, 100]</td>\n",
       "      <td>{'hidden_layer_sizes': [100, 100, 100]}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.829966</td>\n",
       "      <td>0.067394</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       0.301975         0.001292         0.700337          0.728956   \n",
       "1       0.673058         0.001334         0.821549          0.854097   \n",
       "2       0.775510         0.001565         0.753086          0.780022   \n",
       "3       0.487056         0.001599         0.756453          0.768238   \n",
       "4       0.537638         0.001363         0.755331          0.775533   \n",
       "5       0.416390         0.001489         0.810325          0.835578   \n",
       "6       0.678461         0.001893         0.812570          0.837262   \n",
       "7       1.277355         0.005312         0.815937          0.846240   \n",
       "8       0.581189         0.002728         0.821549          0.838384   \n",
       "\n",
       "  param_hidden_layer_sizes                                   params  \\\n",
       "0                     [50]             {'hidden_layer_sizes': [50]}   \n",
       "1                    [100]            {'hidden_layer_sizes': [100]}   \n",
       "2                    [200]            {'hidden_layer_sizes': [200]}   \n",
       "3                 [50, 50]         {'hidden_layer_sizes': [50, 50]}   \n",
       "4                [50, 100]        {'hidden_layer_sizes': [50, 100]}   \n",
       "5                [100, 50]        {'hidden_layer_sizes': [100, 50]}   \n",
       "6               [100, 100]       {'hidden_layer_sizes': [100, 100]}   \n",
       "7               [200, 200]       {'hidden_layer_sizes': [200, 200]}   \n",
       "8          [100, 100, 100]  {'hidden_layer_sizes': [100, 100, 100]}   \n",
       "\n",
       "   rank_test_score  split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0                9           0.818182            0.865320           0.616162   \n",
       "1                1           0.811448            0.872054           0.821549   \n",
       "2                8           0.818182            0.868687           0.821549   \n",
       "3                6           0.622896            0.627946           0.814815   \n",
       "4                7           0.804714            0.860269           0.821549   \n",
       "5                5           0.804714            0.848485           0.791246   \n",
       "6                4           0.784512            0.833333           0.814815   \n",
       "7                3           0.784512            0.853535           0.828283   \n",
       "8                1           0.808081            0.848485           0.831650   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.616162           0.666667            0.705387      0.344140   \n",
       "1            0.845118           0.831650            0.845118      0.115879   \n",
       "2            0.853535           0.619529            0.617845      0.510256   \n",
       "3            0.826599           0.831650            0.850168      0.341058   \n",
       "4            0.829966           0.639731            0.636364      0.377539   \n",
       "5            0.816498           0.835017            0.841751      0.054002   \n",
       "6            0.824916           0.838384            0.853535      0.256673   \n",
       "7            0.831650           0.835017            0.853535      0.477167   \n",
       "8            0.836700           0.824916            0.829966      0.067394   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000218        0.085842         0.103075  \n",
       "1        0.000377        0.008247         0.012698  \n",
       "2        0.000037        0.094450         0.114843  \n",
       "3        0.000420        0.094689         0.099667  \n",
       "4        0.000051        0.082030         0.099182  \n",
       "5        0.000147        0.018305         0.013769  \n",
       "6        0.000245        0.022050         0.012010  \n",
       "7        0.000829        0.022391         0.010317  \n",
       "8        0.000327        0.009912         0.007653  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 hidden layers with 100 units each seem to be the best architecture, giving a test accuracy of 0.82 and not overfitting very much (small difference between train score and test score). We'll keep this architecture and optimize other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=[100, 100, 100], learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'activation': ['logistic', 'tanh', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=[100, 100, 100])\n",
    "clf = GridSearchCV(mlp, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.198135</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.815937</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>logistic</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'logistic', 'solver': 'lbfgs'}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.856902</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.307985</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>0.005992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.275103</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>logistic</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'logistic', 'solver': 'sgd'}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.027726</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.165753</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>logistic</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'logistic', 'solver': 'adam'}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.416595</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.856902</td>\n",
       "      <td>tanh</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'tanh', 'solver': 'lbfgs'}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.107609</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.017168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.136373</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.687991</td>\n",
       "      <td>tanh</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'solver': 'sgd'}</td>\n",
       "      <td>7</td>\n",
       "      <td>0.626263</td>\n",
       "      <td>0.627946</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.624579</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>1.367824</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.078676</td>\n",
       "      <td>0.087308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.853242</td>\n",
       "      <td>0.007210</td>\n",
       "      <td>0.819304</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>tanh</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'solver': 'adam'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.212195</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.003174</td>\n",
       "      <td>0.015852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.212468</td>\n",
       "      <td>0.003659</td>\n",
       "      <td>0.819304</td>\n",
       "      <td>0.832211</td>\n",
       "      <td>relu</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>{'activation': 'relu', 'solver': 'lbfgs'}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.293463</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.893524</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.772166</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>relu</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'solver': 'sgd'}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.077462</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.020756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.686901</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.841190</td>\n",
       "      <td>relu</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'solver': 'adam'}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.231025</td>\n",
       "      <td>0.000536</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       5.198135         0.006849         0.815937          0.851852   \n",
       "1       0.275103         0.005554         0.616162          0.616162   \n",
       "2       0.165753         0.007802         0.616162          0.616162   \n",
       "3       5.416595         0.006964         0.810325          0.856902   \n",
       "4       1.136373         0.006667         0.676768          0.687991   \n",
       "5       0.853242         0.007210         0.819304          0.838384   \n",
       "6       4.212468         0.003659         0.819304          0.832211   \n",
       "7       1.893524         0.002494         0.772166          0.797980   \n",
       "8       0.686901         0.002791         0.812570          0.841190   \n",
       "\n",
       "  param_activation param_solver  \\\n",
       "0         logistic        lbfgs   \n",
       "1         logistic          sgd   \n",
       "2         logistic         adam   \n",
       "3             tanh        lbfgs   \n",
       "4             tanh          sgd   \n",
       "5             tanh         adam   \n",
       "6             relu        lbfgs   \n",
       "7             relu          sgd   \n",
       "8             relu         adam   \n",
       "\n",
       "                                          params  rank_test_score  \\\n",
       "0  {'activation': 'logistic', 'solver': 'lbfgs'}                3   \n",
       "1    {'activation': 'logistic', 'solver': 'sgd'}                8   \n",
       "2   {'activation': 'logistic', 'solver': 'adam'}                8   \n",
       "3      {'activation': 'tanh', 'solver': 'lbfgs'}                5   \n",
       "4        {'activation': 'tanh', 'solver': 'sgd'}                7   \n",
       "5       {'activation': 'tanh', 'solver': 'adam'}                1   \n",
       "6      {'activation': 'relu', 'solver': 'lbfgs'}                1   \n",
       "7        {'activation': 'relu', 'solver': 'sgd'}                6   \n",
       "8       {'activation': 'relu', 'solver': 'adam'}                4   \n",
       "\n",
       "   split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0           0.801347            0.856902           0.831650   \n",
       "1           0.616162            0.616162           0.616162   \n",
       "2           0.616162            0.616162           0.616162   \n",
       "3           0.801347            0.880471           0.821549   \n",
       "4           0.626263            0.627946           0.616162   \n",
       "5           0.814815            0.860269           0.821549   \n",
       "6           0.824916            0.819865           0.811448   \n",
       "7           0.781145            0.794613           0.754209   \n",
       "8           0.774411            0.838384           0.828283   \n",
       "\n",
       "   split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0            0.855219           0.814815            0.843434      0.307985   \n",
       "1            0.616162           0.616162            0.616162      0.027726   \n",
       "2            0.616162           0.616162            0.616162      0.019561   \n",
       "3            0.840067           0.808081            0.850168      0.107609   \n",
       "4            0.624579           0.787879            0.811448      1.367824   \n",
       "5            0.823232           0.821549            0.831650      0.212195   \n",
       "6            0.833333           0.821549            0.843434      0.293463   \n",
       "7            0.774411           0.781145            0.824916      0.077462   \n",
       "8            0.838384           0.835017            0.846801      0.231025   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.000965        0.012397         0.005992  \n",
       "1        0.000711        0.000000         0.000000  \n",
       "2        0.000335        0.000000         0.000000  \n",
       "3        0.000488        0.008399         0.017168  \n",
       "4        0.000698        0.078676         0.087308  \n",
       "5        0.000862        0.003174         0.015852  \n",
       "6        0.000454        0.005723         0.009655  \n",
       "7        0.000171        0.012698         0.020756  \n",
       "8        0.000536        0.027122         0.003968  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is still around 0.82, the best parameters being 'tanh' for the activation function and 'adam' for the solver, which are the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no significant improvement over the baseline after all this feature encoding and hyperparameter tuning, I will just use the data without any encoding (as I did for the baseline) and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.drop('Survived', axis=1).values\n",
    "y = data['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "accuracies = dict()\n",
    "for clf_name in clfs:\n",
    "    clf = clfs[clf_name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracies[clf_name] = clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada': 0.7988826815642458,\n",
       " 'dtc': 0.79329608938547491,\n",
       " 'gbc': 0.81564245810055869,\n",
       " 'gnb': 0.72067039106145248,\n",
       " 'lr': 0.77653631284916202,\n",
       " 'mlp1': 0.65363128491620115,\n",
       " 'mlp2': 0.65363128491620115,\n",
       " 'mnb': 0.58100558659217882,\n",
       " 'rfc': 0.82681564245810057,\n",
       " 'svm1': 0.77653631284916202,\n",
       " 'svm2': 0.6033519553072626,\n",
       " 'svm3': 0.5977653631284916}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier seems to perform best, so I'll do some hyperparameter optimization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.823793</td>\n",
       "      <td>0.982043</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164406</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.997194</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.013489</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.319486</td>\n",
       "      <td>0.018740</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.044562</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.476863</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.606382</td>\n",
       "      <td>0.039708</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006615</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036963</td>\n",
       "      <td>0.003004</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.874860</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.016628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.134726</td>\n",
       "      <td>0.008466</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.875982</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.009748</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>0.012772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.255768</td>\n",
       "      <td>0.016443</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.882716</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.029094</td>\n",
       "      <td>0.006496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.382978</td>\n",
       "      <td>0.024344</td>\n",
       "      <td>0.806958</td>\n",
       "      <td>0.886644</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>41</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.037928</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.511307</td>\n",
       "      <td>0.032052</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.884961</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.032100</td>\n",
       "      <td>0.007571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.832772</td>\n",
       "      <td>0.951740</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.946128</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.139215</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.030981</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.285357</td>\n",
       "      <td>0.018093</td>\n",
       "      <td>0.826038</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.951178</td>\n",
       "      <td>0.004182</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.010997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.420328</td>\n",
       "      <td>0.026509</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.961279</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.028570</td>\n",
       "      <td>0.007571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.558369</td>\n",
       "      <td>0.035936</td>\n",
       "      <td>0.832772</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.034363</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.985971</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.986532</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>0.003459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.165259</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>0.997194</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.330762</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.817059</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007208</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.021354</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.491829</td>\n",
       "      <td>0.027760</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002415</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.691885</td>\n",
       "      <td>0.035545</td>\n",
       "      <td>0.826038</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054815</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.029139</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.744108</td>\n",
       "      <td>0.873737</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.035099</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.140044</td>\n",
       "      <td>0.008560</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.870932</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.005723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.278673</td>\n",
       "      <td>0.016376</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.877104</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.877104</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.035099</td>\n",
       "      <td>0.007142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.421381</td>\n",
       "      <td>0.024415</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.033332</td>\n",
       "      <td>0.009014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.557694</td>\n",
       "      <td>0.032168</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.881594</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.032529</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.948373</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>58</td>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.941077</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.008054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.154926</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>0.830527</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.008584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.311580</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.961841</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.976431</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.470991</td>\n",
       "      <td>0.026252</td>\n",
       "      <td>0.832772</td>\n",
       "      <td>0.961279</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.976431</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.010736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.620940</td>\n",
       "      <td>0.034602</td>\n",
       "      <td>0.823793</td>\n",
       "      <td>0.960718</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.012772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.031164</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.152197</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.021821</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.301287</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.451596</td>\n",
       "      <td>0.027912</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.601864</td>\n",
       "      <td>0.036966</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.026695</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.874860</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.872054</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.013208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.131490</td>\n",
       "      <td>0.008759</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.889450</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.880471</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.023056</td>\n",
       "      <td>0.006496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.261051</td>\n",
       "      <td>0.016510</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.885522</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.032798</td>\n",
       "      <td>0.011744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.388569</td>\n",
       "      <td>0.024429</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.888328</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.872054</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.026512</td>\n",
       "      <td>0.012010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.525798</td>\n",
       "      <td>0.032915</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>0.010191</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.029094</td>\n",
       "      <td>0.009622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.032034</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.006496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.155480</td>\n",
       "      <td>0.009915</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.008584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.315674</td>\n",
       "      <td>0.019670</td>\n",
       "      <td>0.823793</td>\n",
       "      <td>0.968575</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.961279</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.030315</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.008054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.442664</td>\n",
       "      <td>0.027066</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.021354</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.585894</td>\n",
       "      <td>0.035551</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.970819</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.038311</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>58</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.024019</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.196383</td>\n",
       "      <td>0.010554</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.017675</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.386125</td>\n",
       "      <td>0.019532</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.572415</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.761125</td>\n",
       "      <td>0.037708</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.998316</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.996633</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.033276</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.007142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.153368</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.882155</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.868687</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.032567</td>\n",
       "      <td>0.009622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.301874</td>\n",
       "      <td>0.016752</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.895623</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.875421</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.008584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.460621</td>\n",
       "      <td>0.025680</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>39</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.897306</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.890572</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.007240</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.023967</td>\n",
       "      <td>0.014547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.611130</td>\n",
       "      <td>0.033327</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.881594</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>39</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.892256</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.028702</td>\n",
       "      <td>0.012772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.034903</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>0.965769</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.181507</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.965208</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>16</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.978114</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.961279</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.009357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.363324</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.829405</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.020997</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.539266</td>\n",
       "      <td>0.027828</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.962963</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.008837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.720149</td>\n",
       "      <td>0.036231</td>\n",
       "      <td>0.826038</td>\n",
       "      <td>0.967452</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.026417</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.040844         0.003511         0.823793          0.982043   \n",
       "1        0.164406         0.011589         0.818182          0.997194   \n",
       "2        0.319486         0.018740         0.821549          0.998316   \n",
       "3        0.476863         0.029563         0.822671          0.998316   \n",
       "4        0.606382         0.039708         0.828283          0.998316   \n",
       "5        0.036963         0.003004         0.800224          0.874860   \n",
       "6        0.134726         0.008466         0.801347          0.875982   \n",
       "7        0.255768         0.016443         0.804714          0.882716   \n",
       "8        0.382978         0.024344         0.806958          0.886644   \n",
       "9        0.511307         0.032052         0.810325          0.884961   \n",
       "10       0.027630         0.002264         0.832772          0.951740   \n",
       "11       0.139215         0.009439         0.831650          0.964646   \n",
       "12       0.285357         0.018093         0.826038          0.964646   \n",
       "13       0.420328         0.026509         0.821549          0.967452   \n",
       "14       0.558369         0.035936         0.832772          0.967452   \n",
       "15       0.034363         0.002411         0.810325          0.985971   \n",
       "16       0.165259         0.010357         0.822671          0.997194   \n",
       "17       0.330762         0.018524         0.817059          0.998316   \n",
       "18       0.491829         0.027760         0.828283          0.998316   \n",
       "19       0.691885         0.035545         0.826038          0.998316   \n",
       "20       0.029139         0.002227         0.791246          0.863636   \n",
       "21       0.140044         0.008560         0.804714          0.870932   \n",
       "22       0.278673         0.016376         0.811448          0.882155   \n",
       "23       0.421381         0.024415         0.804714          0.882155   \n",
       "24       0.557694         0.032168         0.803591          0.881594   \n",
       "25       0.032529         0.002487         0.795735          0.948373   \n",
       "26       0.154926         0.009240         0.830527          0.957912   \n",
       "27       0.311580         0.017766         0.831650          0.961841   \n",
       "28       0.470991         0.026252         0.832772          0.961279   \n",
       "29       0.620940         0.034602         0.823793          0.960718   \n",
       "30       0.031164         0.002584         0.810325          0.998316   \n",
       "31       0.152197         0.009830         0.814815          0.998316   \n",
       "32       0.301287         0.018832         0.809203          0.998316   \n",
       "33       0.451596         0.027912         0.811448          0.998316   \n",
       "34       0.601864         0.036966         0.810325          0.998316   \n",
       "35       0.026695         0.002494         0.797980          0.874860   \n",
       "36       0.131490         0.008759         0.800224          0.889450   \n",
       "37       0.261051         0.016510         0.803591          0.885522   \n",
       "38       0.388569         0.024429         0.804714          0.888328   \n",
       "39       0.525798         0.032915         0.801347          0.887205   \n",
       "40       0.032034         0.002809         0.811448          0.970819   \n",
       "41       0.155480         0.009915         0.814815          0.968013   \n",
       "42       0.315674         0.019670         0.823793          0.968575   \n",
       "43       0.442664         0.027066         0.827160          0.970819   \n",
       "44       0.585894         0.035551         0.831650          0.970819   \n",
       "45       0.038311         0.002527         0.795735          0.998316   \n",
       "46       0.196383         0.010554         0.812570          0.998316   \n",
       "47       0.386125         0.019532         0.802469          0.998316   \n",
       "48       0.572415         0.027908         0.809203          0.998316   \n",
       "49       0.761125         0.037708         0.802469          0.998316   \n",
       "50       0.033276         0.002338         0.802469          0.870370   \n",
       "51       0.153368         0.008825         0.803591          0.882155   \n",
       "52       0.301874         0.016752         0.804714          0.887205   \n",
       "53       0.460621         0.025680         0.808081          0.883838   \n",
       "54       0.611130         0.033327         0.808081          0.881594   \n",
       "55       0.034903         0.002318         0.822671          0.965769   \n",
       "56       0.181507         0.009880         0.824916          0.965208   \n",
       "57       0.363324         0.018657         0.829405          0.967452   \n",
       "58       0.539266         0.027828         0.828283          0.967452   \n",
       "59       0.720149         0.036231         0.826038          0.967452   \n",
       "\n",
       "   param_bootstrap param_criterion param_max_depth param_n_estimators  \\\n",
       "0             True            gini            None                 10   \n",
       "1             True            gini            None                 50   \n",
       "2             True            gini            None                100   \n",
       "3             True            gini            None                150   \n",
       "4             True            gini            None                200   \n",
       "5             True            gini               5                 10   \n",
       "6             True            gini               5                 50   \n",
       "7             True            gini               5                100   \n",
       "8             True            gini               5                150   \n",
       "9             True            gini               5                200   \n",
       "10            True            gini              10                 10   \n",
       "11            True            gini              10                 50   \n",
       "12            True            gini              10                100   \n",
       "13            True            gini              10                150   \n",
       "14            True            gini              10                200   \n",
       "15            True         entropy            None                 10   \n",
       "16            True         entropy            None                 50   \n",
       "17            True         entropy            None                100   \n",
       "18            True         entropy            None                150   \n",
       "19            True         entropy            None                200   \n",
       "20            True         entropy               5                 10   \n",
       "21            True         entropy               5                 50   \n",
       "22            True         entropy               5                100   \n",
       "23            True         entropy               5                150   \n",
       "24            True         entropy               5                200   \n",
       "25            True         entropy              10                 10   \n",
       "26            True         entropy              10                 50   \n",
       "27            True         entropy              10                100   \n",
       "28            True         entropy              10                150   \n",
       "29            True         entropy              10                200   \n",
       "30           False            gini            None                 10   \n",
       "31           False            gini            None                 50   \n",
       "32           False            gini            None                100   \n",
       "33           False            gini            None                150   \n",
       "34           False            gini            None                200   \n",
       "35           False            gini               5                 10   \n",
       "36           False            gini               5                 50   \n",
       "37           False            gini               5                100   \n",
       "38           False            gini               5                150   \n",
       "39           False            gini               5                200   \n",
       "40           False            gini              10                 10   \n",
       "41           False            gini              10                 50   \n",
       "42           False            gini              10                100   \n",
       "43           False            gini              10                150   \n",
       "44           False            gini              10                200   \n",
       "45           False         entropy            None                 10   \n",
       "46           False         entropy            None                 50   \n",
       "47           False         entropy            None                100   \n",
       "48           False         entropy            None                150   \n",
       "49           False         entropy            None                200   \n",
       "50           False         entropy               5                 10   \n",
       "51           False         entropy               5                 50   \n",
       "52           False         entropy               5                100   \n",
       "53           False         entropy               5                150   \n",
       "54           False         entropy               5                200   \n",
       "55           False         entropy              10                 10   \n",
       "56           False         entropy              10                 50   \n",
       "57           False         entropy              10                100   \n",
       "58           False         entropy              10                150   \n",
       "59           False         entropy              10                200   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {'bootstrap': True, 'criterion': 'gini', 'max_...               17   \n",
       "1   {'bootstrap': True, 'criterion': 'gini', 'max_...               25   \n",
       "2   {'bootstrap': True, 'criterion': 'gini', 'max_...               23   \n",
       "3   {'bootstrap': True, 'criterion': 'gini', 'max_...               20   \n",
       "4   {'bootstrap': True, 'criterion': 'gini', 'max_...                9   \n",
       "5   {'bootstrap': True, 'criterion': 'gini', 'max_...               55   \n",
       "6   {'bootstrap': True, 'criterion': 'gini', 'max_...               53   \n",
       "7   {'bootstrap': True, 'criterion': 'gini', 'max_...               42   \n",
       "8   {'bootstrap': True, 'criterion': 'gini', 'max_...               41   \n",
       "9   {'bootstrap': True, 'criterion': 'gini', 'max_...               33   \n",
       "10  {'bootstrap': True, 'criterion': 'gini', 'max_...                1   \n",
       "11  {'bootstrap': True, 'criterion': 'gini', 'max_...                4   \n",
       "12  {'bootstrap': True, 'criterion': 'gini', 'max_...               13   \n",
       "13  {'bootstrap': True, 'criterion': 'gini', 'max_...               23   \n",
       "14  {'bootstrap': True, 'criterion': 'gini', 'max_...                1   \n",
       "15  {'bootstrap': True, 'criterion': 'entropy', 'm...               33   \n",
       "16  {'bootstrap': True, 'criterion': 'entropy', 'm...               20   \n",
       "17  {'bootstrap': True, 'criterion': 'entropy', 'm...               26   \n",
       "18  {'bootstrap': True, 'criterion': 'entropy', 'm...                9   \n",
       "19  {'bootstrap': True, 'criterion': 'entropy', 'm...               13   \n",
       "20  {'bootstrap': True, 'criterion': 'entropy', 'm...               60   \n",
       "21  {'bootstrap': True, 'criterion': 'entropy', 'm...               42   \n",
       "22  {'bootstrap': True, 'criterion': 'entropy', 'm...               30   \n",
       "23  {'bootstrap': True, 'criterion': 'entropy', 'm...               42   \n",
       "24  {'bootstrap': True, 'criterion': 'entropy', 'm...               47   \n",
       "25  {'bootstrap': True, 'criterion': 'entropy', 'm...               58   \n",
       "26  {'bootstrap': True, 'criterion': 'entropy', 'm...                7   \n",
       "27  {'bootstrap': True, 'criterion': 'entropy', 'm...                4   \n",
       "28  {'bootstrap': True, 'criterion': 'entropy', 'm...                1   \n",
       "29  {'bootstrap': True, 'criterion': 'entropy', 'm...               17   \n",
       "30  {'bootstrap': False, 'criterion': 'gini', 'max...               33   \n",
       "31  {'bootstrap': False, 'criterion': 'gini', 'max...               27   \n",
       "32  {'bootstrap': False, 'criterion': 'gini', 'max...               37   \n",
       "33  {'bootstrap': False, 'criterion': 'gini', 'max...               30   \n",
       "34  {'bootstrap': False, 'criterion': 'gini', 'max...               33   \n",
       "35  {'bootstrap': False, 'criterion': 'gini', 'max...               57   \n",
       "36  {'bootstrap': False, 'criterion': 'gini', 'max...               55   \n",
       "37  {'bootstrap': False, 'criterion': 'gini', 'max...               47   \n",
       "38  {'bootstrap': False, 'criterion': 'gini', 'max...               42   \n",
       "39  {'bootstrap': False, 'criterion': 'gini', 'max...               53   \n",
       "40  {'bootstrap': False, 'criterion': 'gini', 'max...               30   \n",
       "41  {'bootstrap': False, 'criterion': 'gini', 'max...               27   \n",
       "42  {'bootstrap': False, 'criterion': 'gini', 'max...               17   \n",
       "43  {'bootstrap': False, 'criterion': 'gini', 'max...               12   \n",
       "44  {'bootstrap': False, 'criterion': 'gini', 'max...                4   \n",
       "45  {'bootstrap': False, 'criterion': 'entropy', '...               58   \n",
       "46  {'bootstrap': False, 'criterion': 'entropy', '...               29   \n",
       "47  {'bootstrap': False, 'criterion': 'entropy', '...               50   \n",
       "48  {'bootstrap': False, 'criterion': 'entropy', '...               37   \n",
       "49  {'bootstrap': False, 'criterion': 'entropy', '...               50   \n",
       "50  {'bootstrap': False, 'criterion': 'entropy', '...               50   \n",
       "51  {'bootstrap': False, 'criterion': 'entropy', '...               47   \n",
       "52  {'bootstrap': False, 'criterion': 'entropy', '...               42   \n",
       "53  {'bootstrap': False, 'criterion': 'entropy', '...               39   \n",
       "54  {'bootstrap': False, 'criterion': 'entropy', '...               39   \n",
       "55  {'bootstrap': False, 'criterion': 'entropy', '...               20   \n",
       "56  {'bootstrap': False, 'criterion': 'entropy', '...               16   \n",
       "57  {'bootstrap': False, 'criterion': 'entropy', '...                8   \n",
       "58  {'bootstrap': False, 'criterion': 'entropy', '...                9   \n",
       "59  {'bootstrap': False, 'criterion': 'entropy', '...               13   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.797980            0.983165           0.841751   \n",
       "1            0.784512            0.996633           0.838384   \n",
       "2            0.791246            0.998316           0.818182   \n",
       "3            0.791246            0.998316           0.828283   \n",
       "4            0.808081            0.998316           0.841751   \n",
       "5            0.757576            0.890572           0.818182   \n",
       "6            0.754209            0.893939           0.828283   \n",
       "7            0.764310            0.885522           0.831650   \n",
       "8            0.754209            0.892256           0.841751   \n",
       "9            0.767677            0.895623           0.845118   \n",
       "10           0.794613            0.956229           0.838384   \n",
       "11           0.787879            0.974747           0.855219   \n",
       "12           0.794613            0.978114           0.841751   \n",
       "13           0.781145            0.978114           0.841751   \n",
       "14           0.804714            0.979798           0.835017   \n",
       "15           0.797980            0.989899           0.811448   \n",
       "16           0.791246            0.996633           0.835017   \n",
       "17           0.787879            0.998316           0.824916   \n",
       "18           0.797980            0.998316           0.851852   \n",
       "19           0.794613            0.998316           0.838384   \n",
       "20           0.744108            0.873737           0.828283   \n",
       "21           0.760943            0.878788           0.821549   \n",
       "22           0.764310            0.892256           0.848485   \n",
       "23           0.757576            0.892256           0.828283   \n",
       "24           0.764310            0.893939           0.824916   \n",
       "25           0.747475            0.959596           0.814815   \n",
       "26           0.808081            0.969697           0.845118   \n",
       "27           0.797980            0.976431           0.845118   \n",
       "28           0.811448            0.976431           0.841751   \n",
       "29           0.781145            0.978114           0.841751   \n",
       "30           0.794613            0.998316           0.824916   \n",
       "31           0.794613            0.998316           0.804714   \n",
       "32           0.784512            0.998316           0.811448   \n",
       "33           0.797980            0.998316           0.808081   \n",
       "34           0.791246            0.998316           0.811448   \n",
       "35           0.754209            0.892256           0.814815   \n",
       "36           0.767677            0.895623           0.818182   \n",
       "37           0.757576            0.898990           0.821549   \n",
       "38           0.767677            0.900673           0.818182   \n",
       "39           0.760943            0.898990           0.814815   \n",
       "40           0.781145            0.979798           0.814815   \n",
       "41           0.784512            0.979798           0.818182   \n",
       "42           0.797980            0.979798           0.828283   \n",
       "43           0.797980            0.979798           0.835017   \n",
       "44           0.804714            0.981481           0.835017   \n",
       "45           0.771044            0.998316           0.787879   \n",
       "46           0.787879            0.998316           0.821549   \n",
       "47           0.774411            0.998316           0.814815   \n",
       "48           0.791246            0.998316           0.814815   \n",
       "49           0.777778            0.998316           0.804714   \n",
       "50           0.774411            0.875421           0.821549   \n",
       "51           0.757576            0.890572           0.828283   \n",
       "52           0.771044            0.895623           0.821549   \n",
       "53           0.774411            0.897306           0.821549   \n",
       "54           0.767677            0.892256           0.831650   \n",
       "55           0.784512            0.978114           0.824916   \n",
       "56           0.787879            0.978114           0.831650   \n",
       "57           0.801347            0.979798           0.835017   \n",
       "58           0.801347            0.979798           0.824916   \n",
       "59           0.791246            0.979798           0.831650   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.981481           0.831650            0.981481      0.007544   \n",
       "1             0.996633           0.831650            0.998316      0.013489   \n",
       "2             0.996633           0.855219            1.000000      0.044562   \n",
       "3             0.996633           0.848485            1.000000      0.051385   \n",
       "4             0.996633           0.835017            1.000000      0.006615   \n",
       "5             0.882155           0.824916            0.851852      0.000870   \n",
       "6             0.868687           0.821549            0.865320      0.009748   \n",
       "7             0.888889           0.818182            0.873737      0.000710   \n",
       "8             0.883838           0.824916            0.883838      0.001965   \n",
       "9             0.880471           0.818182            0.878788      0.000636   \n",
       "10            0.946128           0.865320            0.952862      0.000119   \n",
       "11            0.962963           0.851852            0.956229      0.003769   \n",
       "12            0.964646           0.841751            0.951178      0.004182   \n",
       "13            0.962963           0.841751            0.961279      0.005375   \n",
       "14            0.964646           0.858586            0.957912      0.005738   \n",
       "15            0.981481           0.821549            0.986532      0.000292   \n",
       "16            0.996633           0.841751            0.998316      0.001728   \n",
       "17            0.996633           0.838384            1.000000      0.007208   \n",
       "18            0.996633           0.835017            1.000000      0.002415   \n",
       "19            0.996633           0.845118            1.000000      0.054815   \n",
       "20            0.861953           0.801347            0.855219      0.000475   \n",
       "21            0.865320           0.831650            0.868687      0.000133   \n",
       "22            0.877104           0.821549            0.877104      0.002335   \n",
       "23            0.883838           0.828283            0.870370      0.000354   \n",
       "24            0.880471           0.821549            0.870370      0.002551   \n",
       "25            0.941077           0.824916            0.944444      0.000429   \n",
       "26            0.954545           0.838384            0.949495      0.001376   \n",
       "27            0.959596           0.851852            0.949495      0.002331   \n",
       "28            0.954545           0.845118            0.952862      0.002472   \n",
       "29            0.956229           0.848485            0.947811      0.003499   \n",
       "30            0.996633           0.811448            1.000000      0.000489   \n",
       "31            0.996633           0.845118            1.000000      0.001047   \n",
       "32            0.996633           0.831650            1.000000      0.003040   \n",
       "33            0.996633           0.828283            1.000000      0.005755   \n",
       "34            0.996633           0.828283            1.000000      0.004005   \n",
       "35            0.872054           0.824916            0.860269      0.000092   \n",
       "36            0.892256           0.814815            0.880471      0.000407   \n",
       "37            0.887205           0.831650            0.870370      0.001002   \n",
       "38            0.892256           0.828283            0.872054      0.001866   \n",
       "39            0.887205           0.828283            0.875421      0.010191   \n",
       "40            0.964646           0.838384            0.968013      0.000479   \n",
       "41            0.959596           0.841751            0.964646      0.001212   \n",
       "42            0.961279           0.845118            0.964646      0.030315   \n",
       "43            0.962963           0.848485            0.969697      0.004655   \n",
       "44            0.962963           0.855219            0.968013      0.004086   \n",
       "45            0.996633           0.828283            1.000000      0.001047   \n",
       "46            0.996633           0.828283            1.000000      0.002739   \n",
       "47            0.996633           0.818182            1.000000      0.009083   \n",
       "48            0.996633           0.821549            1.000000      0.006921   \n",
       "49            0.996633           0.824916            1.000000      0.008925   \n",
       "50            0.875421           0.811448            0.860269      0.002004   \n",
       "51            0.887205           0.824916            0.868687      0.001694   \n",
       "52            0.890572           0.821549            0.875421      0.004213   \n",
       "53            0.890572           0.828283            0.863636      0.007240   \n",
       "54            0.888889           0.824916            0.863636      0.004179   \n",
       "55            0.962963           0.858586            0.956229      0.000508   \n",
       "56            0.961279           0.855219            0.956229      0.003700   \n",
       "57            0.964646           0.851852            0.957912      0.001749   \n",
       "58            0.962963           0.858586            0.959596      0.009359   \n",
       "59            0.964646           0.855219            0.957912      0.008369   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.001515        0.018713         0.000794  \n",
       "1         0.001313        0.023967         0.000794  \n",
       "2         0.000114        0.026225         0.001375  \n",
       "3         0.002466        0.023702         0.001375  \n",
       "4         0.004371        0.014547         0.001375  \n",
       "5         0.000482        0.030282         0.016628  \n",
       "6         0.000082        0.033445         0.012772  \n",
       "7         0.000042        0.029094         0.006496  \n",
       "8         0.000095        0.037928         0.003968  \n",
       "9         0.000116        0.032100         0.007571  \n",
       "10        0.000021        0.029137         0.004199  \n",
       "11        0.000197        0.030981         0.007653  \n",
       "12        0.000232        0.022221         0.010997  \n",
       "13        0.000517        0.028570         0.007571  \n",
       "14        0.000456        0.022050         0.009152  \n",
       "15        0.000063        0.009655         0.003459  \n",
       "16        0.000145        0.022391         0.000794  \n",
       "17        0.000534        0.021354         0.001375  \n",
       "18        0.000814        0.022503         0.001375  \n",
       "19        0.000294        0.022391         0.001375  \n",
       "20        0.000027        0.035099         0.007653  \n",
       "21        0.000049        0.031224         0.005723  \n",
       "22        0.000146        0.035099         0.007142  \n",
       "23        0.000061        0.033332         0.009014  \n",
       "24        0.000036        0.027810         0.009655  \n",
       "25        0.000081        0.034373         0.008054  \n",
       "26        0.000216        0.016109         0.008584  \n",
       "27        0.000130        0.023967         0.011111  \n",
       "28        0.000069        0.015141         0.010736  \n",
       "29        0.000086        0.030282         0.012772  \n",
       "30        0.000101        0.012397         0.001375  \n",
       "31        0.000067        0.021821         0.001375  \n",
       "32        0.000318        0.019309         0.001375  \n",
       "33        0.000213        0.012598         0.001375  \n",
       "34        0.000314        0.015141         0.001375  \n",
       "35        0.000034        0.031224         0.013208  \n",
       "36        0.000145        0.023056         0.006496  \n",
       "37        0.000059        0.032798         0.011744  \n",
       "38        0.000170        0.026512         0.012010  \n",
       "39        0.000993        0.029094         0.009622  \n",
       "40        0.000283        0.023489         0.006496  \n",
       "41        0.000249        0.023489         0.008584  \n",
       "42        0.001583        0.019504         0.008054  \n",
       "43        0.000508        0.021354         0.006919  \n",
       "44        0.000485        0.020756         0.007816  \n",
       "45        0.000156        0.024019         0.001375  \n",
       "46        0.000194        0.017675         0.001375  \n",
       "47        0.000576        0.019888         0.001375  \n",
       "48        0.000257        0.012992         0.001375  \n",
       "49        0.000590        0.019309         0.001375  \n",
       "50        0.000024        0.020264         0.007142  \n",
       "51        0.000349        0.032567         0.009622  \n",
       "52        0.000409        0.023808         0.008584  \n",
       "53        0.000538        0.023967         0.014547  \n",
       "54        0.000731        0.028702         0.012772  \n",
       "55        0.000042        0.030282         0.009152  \n",
       "56        0.000441        0.027901         0.009357  \n",
       "57        0.000156        0.020997         0.009152  \n",
       "58        0.000471        0.023489         0.008837  \n",
       "59        0.000476        0.026417         0.009152  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rfc = RandomForestClassifier()\n",
    "clf = GridSearchCV(rfc, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best accuracy is again around 0.82. Now, I will perform another hyperparameter optimization on random forests but by using data encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest hyperparameter optimization with data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = feature_extraction(data)\n",
    "y = data['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_bootstrap</th>\n",
       "      <th>param_criterion</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031458</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>46</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.964646</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.010378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142488</td>\n",
       "      <td>0.010374</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.973625</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.966330</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.018305</td>\n",
       "      <td>0.005555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302570</td>\n",
       "      <td>0.021939</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.484753</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.050861</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>0.013561</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.039123</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.032632</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.028930</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.856902</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.823232</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.013746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.135594</td>\n",
       "      <td>0.009979</td>\n",
       "      <td>0.819304</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.008837</td>\n",
       "      <td>0.012371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.276443</td>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.815937</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.010378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.420002</td>\n",
       "      <td>0.029824</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.852413</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>0.006781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.536764</td>\n",
       "      <td>0.033934</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.041274</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.008361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028581</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.905163</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>40</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.915825</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.900673</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.021472</td>\n",
       "      <td>0.007571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.136688</td>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.916386</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.930976</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.915825</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.011691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.269751</td>\n",
       "      <td>0.018171</td>\n",
       "      <td>0.813692</td>\n",
       "      <td>0.914703</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.930976</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.905724</td>\n",
       "      <td>0.006428</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.020816</td>\n",
       "      <td>0.011528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.401618</td>\n",
       "      <td>0.027259</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.904040</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>0.012371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.536822</td>\n",
       "      <td>0.035826</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.921437</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'gini', 'max_...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.008944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.031577</td>\n",
       "      <td>0.002681</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.960718</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>41</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.957912</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.150917</td>\n",
       "      <td>0.010358</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.305379</td>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>45</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.438021</td>\n",
       "      <td>0.028932</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>47</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.013561</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.591551</td>\n",
       "      <td>0.038306</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.012467</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.026974</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.813692</td>\n",
       "      <td>0.830527</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.813131</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0.012473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.129818</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.008361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.258757</td>\n",
       "      <td>0.017041</td>\n",
       "      <td>0.817059</td>\n",
       "      <td>0.845679</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.860269</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.010408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.385175</td>\n",
       "      <td>0.024859</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.842873</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.826599</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.012010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.507500</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.806958</td>\n",
       "      <td>0.847924</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.004199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.029468</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.905163</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>36</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.902357</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.887205</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.015932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.139165</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.904602</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.893939</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>0.011691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.277050</td>\n",
       "      <td>0.018428</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.913580</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.912458</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.012397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.440607</td>\n",
       "      <td>0.028133</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.904040</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>0.013256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.586242</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.910213</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': True, 'criterion': 'entropy', 'm...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.912458</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.025880</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.016495</td>\n",
       "      <td>0.008399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.029808</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>59</td>\n",
       "      <td>0.750842</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.152329</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.782267</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.008324</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.017459</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.323331</td>\n",
       "      <td>0.021922</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.514597</td>\n",
       "      <td>0.030315</td>\n",
       "      <td>0.782267</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>52</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.049022</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.020264</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.632615</td>\n",
       "      <td>0.039819</td>\n",
       "      <td>0.780022</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.033433</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.025279</td>\n",
       "      <td>0.002386</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.836700</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.035099</td>\n",
       "      <td>0.004956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.120964</td>\n",
       "      <td>0.008712</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.852413</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.009357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.249533</td>\n",
       "      <td>0.019073</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.002742</td>\n",
       "      <td>0.009912</td>\n",
       "      <td>0.009622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.400137</td>\n",
       "      <td>0.027107</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.851291</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.027082</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.005204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.501037</td>\n",
       "      <td>0.032544</td>\n",
       "      <td>0.815937</td>\n",
       "      <td>0.854097</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.865320</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.000486</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.008944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.026767</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.924804</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>39</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.915825</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.010408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.131675</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>0.810325</td>\n",
       "      <td>0.928732</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>23</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.024945</td>\n",
       "      <td>0.011195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.262149</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.929854</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>21</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.025196</td>\n",
       "      <td>0.013905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.392892</td>\n",
       "      <td>0.026706</td>\n",
       "      <td>0.805836</td>\n",
       "      <td>0.932099</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.952862</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.014698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.522913</td>\n",
       "      <td>0.035584</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.933221</td>\n",
       "      <td>False</td>\n",
       "      <td>gini</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'gini', 'max...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.930976</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.022891</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.032161</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.156102</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.311269</td>\n",
       "      <td>0.019687</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.004576</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.009523</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.461892</td>\n",
       "      <td>0.029331</td>\n",
       "      <td>0.775533</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>60</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.010408</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.673070</td>\n",
       "      <td>0.044181</td>\n",
       "      <td>0.780022</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>57</td>\n",
       "      <td>0.760943</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.968013</td>\n",
       "      <td>0.038672</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.005498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.032622</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.805836</td>\n",
       "      <td>0.844557</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>30</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.855219</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.027537</td>\n",
       "      <td>0.008286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.142622</td>\n",
       "      <td>0.009512</td>\n",
       "      <td>0.820426</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.015632</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.299779</td>\n",
       "      <td>0.017129</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.847363</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>37</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.019888</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.410844</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>0.813692</td>\n",
       "      <td>0.845118</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.025623</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.012598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.553410</td>\n",
       "      <td>0.034956</td>\n",
       "      <td>0.826038</td>\n",
       "      <td>0.847363</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.856902</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.846801</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.007571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.806958</td>\n",
       "      <td>0.916947</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.930976</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.905724</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.010498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.155057</td>\n",
       "      <td>0.010313</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.951178</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.013286</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>0.013905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.315560</td>\n",
       "      <td>0.019723</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.928171</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.947811</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.920875</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.915825</td>\n",
       "      <td>0.034425</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>0.014040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.448633</td>\n",
       "      <td>0.027505</td>\n",
       "      <td>0.812570</td>\n",
       "      <td>0.926487</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.941077</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.924242</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.914141</td>\n",
       "      <td>0.005451</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.625310</td>\n",
       "      <td>0.038177</td>\n",
       "      <td>0.815937</td>\n",
       "      <td>0.927609</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'bootstrap': False, 'criterion': 'entropy', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.942761</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.835017</td>\n",
       "      <td>0.917508</td>\n",
       "      <td>0.019475</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.010910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0        0.031458         0.003201         0.789001          0.952862   \n",
       "1        0.142488         0.010374         0.786756          0.973625   \n",
       "2        0.302570         0.021939         0.791246          0.974747   \n",
       "3        0.484753         0.031698         0.793490          0.974747   \n",
       "4        0.609070         0.039123         0.785634          0.974747   \n",
       "5        0.028930         0.002598         0.808081          0.840067   \n",
       "6        0.135594         0.009979         0.819304          0.840067   \n",
       "7        0.276443         0.017412         0.815937          0.850168   \n",
       "8        0.420002         0.029824         0.808081          0.852413   \n",
       "9        0.536764         0.033934         0.818182          0.848485   \n",
       "10       0.028581         0.002566         0.797980          0.905163   \n",
       "11       0.136688         0.009946         0.804714          0.916386   \n",
       "12       0.269751         0.018171         0.813692          0.914703   \n",
       "13       0.401618         0.027259         0.810325          0.919192   \n",
       "14       0.536822         0.035826         0.804714          0.921437   \n",
       "15       0.031577         0.002681         0.794613          0.960718   \n",
       "16       0.150917         0.010358         0.785634          0.974747   \n",
       "17       0.305379         0.019587         0.790123          0.974747   \n",
       "18       0.438021         0.028932         0.786756          0.974747   \n",
       "19       0.591551         0.038306         0.793490          0.974747   \n",
       "20       0.026974         0.002282         0.813692          0.830527   \n",
       "21       0.129818         0.009428         0.804714          0.838384   \n",
       "22       0.258757         0.017041         0.817059          0.845679   \n",
       "23       0.385175         0.024859         0.818182          0.842873   \n",
       "24       0.507500         0.033228         0.806958          0.847924   \n",
       "25       0.029468         0.002766         0.803591          0.905163   \n",
       "26       0.139165         0.009641         0.802469          0.904602   \n",
       "27       0.277050         0.018428         0.820426          0.913580   \n",
       "28       0.440607         0.028133         0.804714          0.910774   \n",
       "29       0.586242         0.036370         0.814815          0.910213   \n",
       "30       0.029808         0.002680         0.777778          0.974747   \n",
       "31       0.152329         0.010755         0.782267          0.974747   \n",
       "32       0.323331         0.021922         0.781145          0.974747   \n",
       "33       0.514597         0.030315         0.782267          0.974747   \n",
       "34       0.632615         0.039819         0.780022          0.974747   \n",
       "35       0.025279         0.002386         0.811448          0.843434   \n",
       "36       0.120964         0.008712         0.812570          0.852413   \n",
       "37       0.249533         0.019073         0.818182          0.846801   \n",
       "38       0.400137         0.027107         0.818182          0.851291   \n",
       "39       0.501037         0.032544         0.815937          0.854097   \n",
       "40       0.026767         0.002359         0.800224          0.924804   \n",
       "41       0.131675         0.009432         0.810325          0.928732   \n",
       "42       0.262149         0.018222         0.811448          0.929854   \n",
       "43       0.392892         0.026706         0.805836          0.932099   \n",
       "44       0.522913         0.035584         0.812570          0.933221   \n",
       "45       0.032161         0.002712         0.785634          0.974747   \n",
       "46       0.156102         0.010417         0.781145          0.974747   \n",
       "47       0.311269         0.019687         0.781145          0.974747   \n",
       "48       0.461892         0.029331         0.775533          0.974747   \n",
       "49       0.673070         0.044181         0.780022          0.974747   \n",
       "50       0.032622         0.002689         0.805836          0.844557   \n",
       "51       0.142622         0.009512         0.820426          0.841751   \n",
       "52       0.299779         0.017129         0.802469          0.847363   \n",
       "53       0.410844         0.025870         0.813692          0.845118   \n",
       "54       0.553410         0.034956         0.826038          0.847363   \n",
       "55       0.036798         0.002936         0.806958          0.916947   \n",
       "56       0.155057         0.010313         0.809203          0.931538   \n",
       "57       0.315560         0.019723         0.814815          0.928171   \n",
       "58       0.448633         0.027505         0.812570          0.926487   \n",
       "59       0.625310         0.038177         0.815937          0.927609   \n",
       "\n",
       "   param_bootstrap param_criterion param_max_depth param_n_estimators  \\\n",
       "0             True            gini            None                 10   \n",
       "1             True            gini            None                 50   \n",
       "2             True            gini            None                100   \n",
       "3             True            gini            None                150   \n",
       "4             True            gini            None                200   \n",
       "5             True            gini               5                 10   \n",
       "6             True            gini               5                 50   \n",
       "7             True            gini               5                100   \n",
       "8             True            gini               5                150   \n",
       "9             True            gini               5                200   \n",
       "10            True            gini              10                 10   \n",
       "11            True            gini              10                 50   \n",
       "12            True            gini              10                100   \n",
       "13            True            gini              10                150   \n",
       "14            True            gini              10                200   \n",
       "15            True         entropy            None                 10   \n",
       "16            True         entropy            None                 50   \n",
       "17            True         entropy            None                100   \n",
       "18            True         entropy            None                150   \n",
       "19            True         entropy            None                200   \n",
       "20            True         entropy               5                 10   \n",
       "21            True         entropy               5                 50   \n",
       "22            True         entropy               5                100   \n",
       "23            True         entropy               5                150   \n",
       "24            True         entropy               5                200   \n",
       "25            True         entropy              10                 10   \n",
       "26            True         entropy              10                 50   \n",
       "27            True         entropy              10                100   \n",
       "28            True         entropy              10                150   \n",
       "29            True         entropy              10                200   \n",
       "30           False            gini            None                 10   \n",
       "31           False            gini            None                 50   \n",
       "32           False            gini            None                100   \n",
       "33           False            gini            None                150   \n",
       "34           False            gini            None                200   \n",
       "35           False            gini               5                 10   \n",
       "36           False            gini               5                 50   \n",
       "37           False            gini               5                100   \n",
       "38           False            gini               5                150   \n",
       "39           False            gini               5                200   \n",
       "40           False            gini              10                 10   \n",
       "41           False            gini              10                 50   \n",
       "42           False            gini              10                100   \n",
       "43           False            gini              10                150   \n",
       "44           False            gini              10                200   \n",
       "45           False         entropy            None                 10   \n",
       "46           False         entropy            None                 50   \n",
       "47           False         entropy            None                100   \n",
       "48           False         entropy            None                150   \n",
       "49           False         entropy            None                200   \n",
       "50           False         entropy               5                 10   \n",
       "51           False         entropy               5                 50   \n",
       "52           False         entropy               5                100   \n",
       "53           False         entropy               5                150   \n",
       "54           False         entropy               5                200   \n",
       "55           False         entropy              10                 10   \n",
       "56           False         entropy              10                 50   \n",
       "57           False         entropy              10                100   \n",
       "58           False         entropy              10                150   \n",
       "59           False         entropy              10                200   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "0   {'bootstrap': True, 'criterion': 'gini', 'max_...               46   \n",
       "1   {'bootstrap': True, 'criterion': 'gini', 'max_...               47   \n",
       "2   {'bootstrap': True, 'criterion': 'gini', 'max_...               44   \n",
       "3   {'bootstrap': True, 'criterion': 'gini', 'max_...               42   \n",
       "4   {'bootstrap': True, 'criterion': 'gini', 'max_...               49   \n",
       "5   {'bootstrap': True, 'criterion': 'gini', 'max_...               26   \n",
       "6   {'bootstrap': True, 'criterion': 'gini', 'max_...                4   \n",
       "7   {'bootstrap': True, 'criterion': 'gini', 'max_...               10   \n",
       "8   {'bootstrap': True, 'criterion': 'gini', 'max_...               26   \n",
       "9   {'bootstrap': True, 'criterion': 'gini', 'max_...                5   \n",
       "10  {'bootstrap': True, 'criterion': 'gini', 'max_...               40   \n",
       "11  {'bootstrap': True, 'criterion': 'gini', 'max_...               32   \n",
       "12  {'bootstrap': True, 'criterion': 'gini', 'max_...               15   \n",
       "13  {'bootstrap': True, 'criterion': 'gini', 'max_...               23   \n",
       "14  {'bootstrap': True, 'criterion': 'gini', 'max_...               32   \n",
       "15  {'bootstrap': True, 'criterion': 'entropy', 'm...               41   \n",
       "16  {'bootstrap': True, 'criterion': 'entropy', 'm...               49   \n",
       "17  {'bootstrap': True, 'criterion': 'entropy', 'm...               45   \n",
       "18  {'bootstrap': True, 'criterion': 'entropy', 'm...               47   \n",
       "19  {'bootstrap': True, 'criterion': 'entropy', 'm...               42   \n",
       "20  {'bootstrap': True, 'criterion': 'entropy', 'm...               15   \n",
       "21  {'bootstrap': True, 'criterion': 'entropy', 'm...               32   \n",
       "22  {'bootstrap': True, 'criterion': 'entropy', 'm...                9   \n",
       "23  {'bootstrap': True, 'criterion': 'entropy', 'm...                5   \n",
       "24  {'bootstrap': True, 'criterion': 'entropy', 'm...               28   \n",
       "25  {'bootstrap': True, 'criterion': 'entropy', 'm...               36   \n",
       "26  {'bootstrap': True, 'criterion': 'entropy', 'm...               37   \n",
       "27  {'bootstrap': True, 'criterion': 'entropy', 'm...                2   \n",
       "28  {'bootstrap': True, 'criterion': 'entropy', 'm...               32   \n",
       "29  {'bootstrap': True, 'criterion': 'entropy', 'm...               13   \n",
       "30  {'bootstrap': False, 'criterion': 'gini', 'max...               59   \n",
       "31  {'bootstrap': False, 'criterion': 'gini', 'max...               52   \n",
       "32  {'bootstrap': False, 'criterion': 'gini', 'max...               54   \n",
       "33  {'bootstrap': False, 'criterion': 'gini', 'max...               52   \n",
       "34  {'bootstrap': False, 'criterion': 'gini', 'max...               57   \n",
       "35  {'bootstrap': False, 'criterion': 'gini', 'max...               21   \n",
       "36  {'bootstrap': False, 'criterion': 'gini', 'max...               18   \n",
       "37  {'bootstrap': False, 'criterion': 'gini', 'max...                5   \n",
       "38  {'bootstrap': False, 'criterion': 'gini', 'max...                5   \n",
       "39  {'bootstrap': False, 'criterion': 'gini', 'max...               10   \n",
       "40  {'bootstrap': False, 'criterion': 'gini', 'max...               39   \n",
       "41  {'bootstrap': False, 'criterion': 'gini', 'max...               23   \n",
       "42  {'bootstrap': False, 'criterion': 'gini', 'max...               21   \n",
       "43  {'bootstrap': False, 'criterion': 'gini', 'max...               30   \n",
       "44  {'bootstrap': False, 'criterion': 'gini', 'max...               18   \n",
       "45  {'bootstrap': False, 'criterion': 'entropy', '...               49   \n",
       "46  {'bootstrap': False, 'criterion': 'entropy', '...               54   \n",
       "47  {'bootstrap': False, 'criterion': 'entropy', '...               54   \n",
       "48  {'bootstrap': False, 'criterion': 'entropy', '...               60   \n",
       "49  {'bootstrap': False, 'criterion': 'entropy', '...               57   \n",
       "50  {'bootstrap': False, 'criterion': 'entropy', '...               30   \n",
       "51  {'bootstrap': False, 'criterion': 'entropy', '...                2   \n",
       "52  {'bootstrap': False, 'criterion': 'entropy', '...               37   \n",
       "53  {'bootstrap': False, 'criterion': 'entropy', '...               15   \n",
       "54  {'bootstrap': False, 'criterion': 'entropy', '...                1   \n",
       "55  {'bootstrap': False, 'criterion': 'entropy', '...               28   \n",
       "56  {'bootstrap': False, 'criterion': 'entropy', '...               25   \n",
       "57  {'bootstrap': False, 'criterion': 'entropy', '...               13   \n",
       "58  {'bootstrap': False, 'criterion': 'entropy', '...               18   \n",
       "59  {'bootstrap': False, 'criterion': 'entropy', '...               10   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0            0.764310            0.964646           0.801347   \n",
       "1            0.767677            0.979798           0.781145   \n",
       "2            0.767677            0.981481           0.797980   \n",
       "3            0.774411            0.981481           0.801347   \n",
       "4            0.760943            0.981481           0.784512   \n",
       "5            0.791246            0.856902           0.831650   \n",
       "6            0.811448            0.855219           0.831650   \n",
       "7            0.794613            0.863636           0.835017   \n",
       "8            0.791246            0.861953           0.818182   \n",
       "9            0.797980            0.860269           0.831650   \n",
       "10           0.781145            0.915825           0.784512   \n",
       "11           0.767677            0.930976           0.811448   \n",
       "12           0.784512            0.930976           0.824916   \n",
       "13           0.784512            0.934343           0.811448   \n",
       "14           0.781145            0.932660           0.804714   \n",
       "15           0.777778            0.974747           0.791246   \n",
       "16           0.777778            0.981481           0.787879   \n",
       "17           0.767677            0.981481           0.797980   \n",
       "18           0.767677            0.981481           0.797980   \n",
       "19           0.774411            0.981481           0.797980   \n",
       "20           0.797980            0.841751           0.821549   \n",
       "21           0.771044            0.850168           0.831650   \n",
       "22           0.811448            0.860269           0.824916   \n",
       "23           0.814815            0.855219           0.831650   \n",
       "24           0.784512            0.853535           0.824916   \n",
       "25           0.777778            0.925926           0.821549   \n",
       "26           0.781145            0.920875           0.808081   \n",
       "27           0.814815            0.929293           0.814815   \n",
       "28           0.771044            0.929293           0.814815   \n",
       "29           0.794613            0.919192           0.814815   \n",
       "30           0.750842            0.981481           0.781145   \n",
       "31           0.757576            0.981481           0.794613   \n",
       "32           0.760943            0.981481           0.794613   \n",
       "33           0.754209            0.981481           0.801347   \n",
       "34           0.757576            0.981481           0.794613   \n",
       "35           0.764310            0.848485           0.848485   \n",
       "36           0.797980            0.865320           0.831650   \n",
       "37           0.808081            0.858586           0.831650   \n",
       "38           0.804714            0.858586           0.835017   \n",
       "39           0.804714            0.865320           0.824916   \n",
       "40           0.760943            0.939394           0.818182   \n",
       "41           0.777778            0.944444           0.814815   \n",
       "42           0.777778            0.949495           0.818182   \n",
       "43           0.777778            0.952862           0.808081   \n",
       "44           0.781145            0.947811           0.821549   \n",
       "45           0.771044            0.981481           0.794613   \n",
       "46           0.764310            0.981481           0.797980   \n",
       "47           0.767677            0.981481           0.787879   \n",
       "48           0.760943            0.981481           0.784512   \n",
       "49           0.760943            0.981481           0.794613   \n",
       "50           0.767677            0.835017           0.831650   \n",
       "51           0.804714            0.850168           0.841751   \n",
       "52           0.774411            0.845118           0.818182   \n",
       "53           0.808081            0.861953           0.831650   \n",
       "54           0.821549            0.856902           0.841751   \n",
       "55           0.797980            0.930976           0.804714   \n",
       "56           0.777778            0.951178           0.821549   \n",
       "57           0.784512            0.947811           0.821549   \n",
       "58           0.781145            0.941077           0.818182   \n",
       "59           0.794613            0.942761           0.818182   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0             0.954545           0.801347            0.939394      0.003374   \n",
       "1             0.974747           0.811448            0.966330      0.002909   \n",
       "2             0.974747           0.808081            0.968013      0.007154   \n",
       "3             0.974747           0.804714            0.968013      0.050861   \n",
       "4             0.974747           0.811448            0.968013      0.032632   \n",
       "5             0.823232           0.801347            0.840067      0.002022   \n",
       "6             0.824916           0.814815            0.840067      0.001737   \n",
       "7             0.838384           0.818182            0.848485      0.012653   \n",
       "8             0.848485           0.814815            0.846801      0.026278   \n",
       "9             0.841751           0.824916            0.843434      0.041274   \n",
       "10            0.900673           0.828283            0.898990      0.000829   \n",
       "11            0.915825           0.835017            0.902357      0.000638   \n",
       "12            0.907407           0.831650            0.905724      0.006428   \n",
       "13            0.919192           0.835017            0.904040      0.006618   \n",
       "14            0.920875           0.828283            0.910774      0.008365   \n",
       "15            0.957912           0.814815            0.949495      0.000313   \n",
       "16            0.974747           0.791246            0.968013      0.001981   \n",
       "17            0.974747           0.804714            0.968013      0.012654   \n",
       "18            0.974747           0.794613            0.968013      0.001584   \n",
       "19            0.974747           0.808081            0.968013      0.012467   \n",
       "20            0.813131           0.821549            0.836700      0.000088   \n",
       "21            0.831650           0.811448            0.833333      0.000733   \n",
       "22            0.836700           0.814815            0.840067      0.000957   \n",
       "23            0.826599           0.808081            0.846801      0.000440   \n",
       "24            0.846801           0.811448            0.843434      0.006241   \n",
       "25            0.902357           0.811448            0.887205      0.000924   \n",
       "26            0.898990           0.818182            0.893939      0.001219   \n",
       "27            0.912458           0.831650            0.898990      0.006190   \n",
       "28            0.904040           0.828283            0.898990      0.020716   \n",
       "29            0.912458           0.835017            0.898990      0.025880   \n",
       "30            0.974747           0.801347            0.968013      0.000235   \n",
       "31            0.974747           0.794613            0.968013      0.008324   \n",
       "32            0.974747           0.787879            0.968013      0.005399   \n",
       "33            0.974747           0.791246            0.968013      0.049022   \n",
       "34            0.974747           0.787879            0.968013      0.033433   \n",
       "35            0.836700           0.821549            0.845118      0.000056   \n",
       "36            0.848485           0.808081            0.843434      0.000970   \n",
       "37            0.835017           0.814815            0.846801      0.003923   \n",
       "38            0.848485           0.814815            0.846801      0.027082   \n",
       "39            0.843434           0.818182            0.853535      0.014616   \n",
       "40            0.919192           0.821549            0.915825      0.000248   \n",
       "41            0.922559           0.838384            0.919192      0.000359   \n",
       "42            0.920875           0.838384            0.919192      0.000408   \n",
       "43            0.922559           0.831650            0.920875      0.001292   \n",
       "44            0.930976           0.835017            0.920875      0.001721   \n",
       "45            0.974747           0.791246            0.968013      0.000131   \n",
       "46            0.974747           0.781145            0.968013      0.002317   \n",
       "47            0.974747           0.787879            0.968013      0.004576   \n",
       "48            0.974747           0.781145            0.968013      0.004076   \n",
       "49            0.974747           0.784512            0.968013      0.038672   \n",
       "50            0.855219           0.818182            0.843434      0.002540   \n",
       "51            0.831650           0.814815            0.843434      0.011304   \n",
       "52            0.851852           0.814815            0.845118      0.018347   \n",
       "53            0.841751           0.801347            0.831650      0.025623   \n",
       "54            0.838384           0.814815            0.846801      0.047345   \n",
       "55            0.914141           0.818182            0.905724      0.001651   \n",
       "56            0.922559           0.828283            0.920875      0.013286   \n",
       "57            0.920875           0.838384            0.915825      0.034425   \n",
       "58            0.924242           0.838384            0.914141      0.005451   \n",
       "59            0.922559           0.835017            0.917508      0.019475   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "0         0.000814        0.017459         0.010378  \n",
       "1         0.000186        0.018305         0.005555  \n",
       "2         0.001970        0.017168         0.005498  \n",
       "3         0.004234        0.013561         0.005498  \n",
       "4         0.001227        0.020634         0.005498  \n",
       "5         0.000513        0.017168         0.013746  \n",
       "6         0.001238        0.008837         0.012371  \n",
       "7         0.000059        0.016571         0.010378  \n",
       "8         0.006045        0.011983         0.006781  \n",
       "9         0.000423        0.014547         0.008361  \n",
       "10        0.000154        0.021472         0.007571  \n",
       "11        0.000309        0.027901         0.011691  \n",
       "12        0.000040        0.020816         0.011528  \n",
       "13        0.000687        0.020634         0.012371  \n",
       "14        0.000236        0.019244         0.008944  \n",
       "15        0.000239        0.015307         0.010498  \n",
       "16        0.000170        0.005723         0.005498  \n",
       "17        0.000778        0.016109         0.005498  \n",
       "18        0.000456        0.013561         0.005498  \n",
       "19        0.000124        0.014108         0.005498  \n",
       "20        0.000007        0.011111         0.012473  \n",
       "21        0.000070        0.025196         0.008361  \n",
       "22        0.000058        0.005723         0.010408  \n",
       "23        0.000526        0.009912         0.012010  \n",
       "24        0.000524        0.016798         0.004199  \n",
       "25        0.000243        0.018713         0.015932  \n",
       "26        0.000149        0.015632         0.011691  \n",
       "27        0.000804        0.007936         0.012397  \n",
       "28        0.000908        0.024435         0.013256  \n",
       "29        0.000799        0.016495         0.008399  \n",
       "30        0.000091        0.020756         0.005498  \n",
       "31        0.000592        0.017459         0.005498  \n",
       "32        0.001297        0.014547         0.005498  \n",
       "33        0.001194        0.020264         0.005498  \n",
       "34        0.000629        0.016109         0.005498  \n",
       "35        0.000065        0.035099         0.004956  \n",
       "36        0.000157        0.014108         0.009357  \n",
       "37        0.002742        0.009912         0.009622  \n",
       "38        0.003173        0.012598         0.005204  \n",
       "39        0.000486        0.008399         0.008944  \n",
       "40        0.000023        0.027810         0.010408  \n",
       "41        0.000152        0.024945         0.011195  \n",
       "42        0.000213        0.025196         0.013905  \n",
       "43        0.000122        0.022050         0.014698  \n",
       "44        0.000488        0.022891         0.011111  \n",
       "45        0.000064        0.010408         0.005498  \n",
       "46        0.000180        0.013746         0.005498  \n",
       "47        0.000187        0.009523         0.005498  \n",
       "48        0.000231        0.010408         0.005498  \n",
       "49        0.004521        0.014108         0.005498  \n",
       "50        0.000131        0.027537         0.008286  \n",
       "51        0.000591        0.015632         0.007653  \n",
       "52        0.000281        0.019888         0.003174  \n",
       "53        0.000287        0.012992         0.012598  \n",
       "54        0.002205        0.011446         0.007571  \n",
       "55        0.000310        0.008399         0.010498  \n",
       "56        0.001023        0.022391         0.013905  \n",
       "57        0.000673        0.022503         0.014040  \n",
       "58        0.000796        0.023702         0.011111  \n",
       "59        0.002358        0.016571         0.010910  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rfc = RandomForestClassifier()\n",
    "clf = GridSearchCV(rfc, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, 0.82!\n",
    "\n",
    "I'll just use the neural network with the hyperparameters tuned before for the final submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering all over again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This article](https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/) opened my eyes to the fact that the names of the passengers are not useless information. On the contrary, they contain the person's title ('Dr', 'Master', 'Capt'...), and we can expect this information to be discriminative with regards to the survival of a person. The article also gives a certain number of feature engineering ideas that I'll be using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to read the data again and perform a different preprocessing, extracting a person's title from its name and adding a few columns to the data by re-doing what was done in the article previously cited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-read the data\n",
    "data = pd.read_csv('data/train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId    891\n",
       "Survived       891\n",
       "Pclass         891\n",
       "Name           891\n",
       "Sex            891\n",
       "Age            714\n",
       "SibSp          891\n",
       "Parch          891\n",
       "Ticket         891\n",
       "Fare           891\n",
       "Cabin          204\n",
       "Embarked       889\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values for the columns 'Age', 'Cabin', and 'Embarked' will be filled the same way as I did in the beginning.\n",
    "\n",
    "I'll work on the 'Name' column now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                              Braund, Mr. Owen Harris\n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...\n",
       "2                               Heikkinen, Miss. Laina\n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)\n",
       "4                             Allen, Mr. William Henry\n",
       "5                                     Moran, Mr. James\n",
       "6                              McCarthy, Mr. Timothy J\n",
       "7                       Palsson, Master. Gosta Leonard\n",
       "8    Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\n",
       "9                  Nasser, Mrs. Nicholas (Adele Achem)\n",
       "Name: Name, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Name'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Name' column seems to follow the following pattern: {last name}, {title}. {first names}\n",
    "\n",
    "I'll use this information to extract the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "for name in data['Name'].str.lower():\n",
    "    right_part = name.split(', ')[1]  # right_part = {title}. {first names}\n",
    "    title = right_part.split('.')[0]\n",
    "    titles.append(title)\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all rows have titles in the 'Name' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " 'sir',\n",
       " 'jonkheer',\n",
       " 'capt',\n",
       " 'dr',\n",
       " 'lady',\n",
       " 'col',\n",
       " 'the countess',\n",
       " 'master',\n",
       " 'mme',\n",
       " 'ms',\n",
       " 'miss',\n",
       " 'rev',\n",
       " 'mr',\n",
       " 'major',\n",
       " 'mrs',\n",
       " 'mlle']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = list(set(titles))\n",
    "print(len(titles))\n",
    "titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to re-order the titles according to their nobility. This certainly won't be a perfect order since I'm not very familiar with these kind of titles. After the re-ordering, I can transform the 'Name' column into an ordinal column, easier to work with for classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = ['unk', 'miss', 'mlle', 'mrs', 'mr', 'ms', 'mme', 'lady', 'dr', 'sir', 'master', 'rev', 'major', 'col', 'capt', 'don', 'jonkheer', 'the countess']\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  8,  9, 12, 11, 13, 10,  5,  3, 15,  7, 14,  6,  1,  0,  2,  4,\n",
       "       16])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_encoder = LabelEncoder()\n",
    "tmp = title_encoder.fit_transform(titles)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now write a function that transforms the name column into an integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_title_from_name(name):\n",
    "    right_part = name.split(', ')[1]  # right_part = {title}. {first names}\n",
    "    title = right_part.split('.')[0]\n",
    "    known_titles = ['unk', 'miss', 'mlle', 'mrs', 'mr', 'ms', 'mme', 'lady', 'dr', 'sir', 'master', 'rev', 'major', 'col', 'capt', 'don', 'jonkheer', 'the countess']\n",
    "    return title if title in known_titles else 'unk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_titles(df):\n",
    "    titles = df['Name'].str.lower().apply(get_title_from_name)\n",
    "    titles = np.array(titles)\n",
    "    df['Title'] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>mrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked Title  \n",
       "0      0         A/5 21171   7.2500   NaN        S    mr  \n",
       "1      0          PC 17599  71.2833   C85        C   mrs  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  miss  \n",
       "3      0            113803  53.1000  C123        S   mrs  \n",
       "4      0            373450   8.0500   NaN        S    mr  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_titles(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to transform the 'Cabin' column into a 'Deck' column, I think that that's where the discriminative information might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cabin_to_deck(cabin):\n",
    "    if cabin == 'UNK':\n",
    "        return 'UNK'\n",
    "    return cabin[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to re-read the data in order to undo the changes above. Then, I'm going to re-define the preprocess() and feature_extraction() functions to implement some feature engineering. After that, I'll test several classifiers; and finally, I'll optimize the hyperparemeters of one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoders, column_values = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(df, test=False):\n",
    "    # fill in the missing values\n",
    "    df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
    "    df['Cabin'].fillna('UNK', inplace=True)\n",
    "    df['Embarked'].fillna('UNK', inplace=True)\n",
    "    \n",
    "    # create columns for feature engineering\n",
    "    # add the 'Title' column\n",
    "    set_titles(df)\n",
    "    \n",
    "    # add a 'FamilySize' column\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch']\n",
    "    \n",
    "    # turn the cabin number into Deck\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'UNK']\n",
    "    df['Deck'] = df['Cabin'].apply(cabin_to_deck)\n",
    "    \n",
    "    # define the lists of column values for categorical data,\n",
    "    # and the corresponding encoders (if train data)\n",
    "    if not test:\n",
    "        for column in ['Sex', 'Embarked', 'Deck']:\n",
    "            column_values[column] = list(set(df[column])) + ['other']\n",
    "            encoders[column] = LabelEncoder()\n",
    "            encoders[column].fit(column_values[column])\n",
    "    \n",
    "    # adjust certain values if test data\n",
    "    if test:\n",
    "        for column in ['Sex', 'Embarked', 'Deck']:\n",
    "            df[column] = df[column].apply(lambda x: x if x in column_values[column] else 'other')\n",
    "    \n",
    "    # drop the 'PassengerId', 'Name' and 'Cabin' columns\n",
    "    df.drop('PassengerId', axis=1, inplace=True)\n",
    "    df.drop('Name', axis=1, inplace=True)\n",
    "    df.drop('Ticket', axis=1, inplace=True)\n",
    "    df.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction(df):\n",
    "    # unchanged columns\n",
    "    X = np.concatenate((df['Age'].values.reshape(-1, 1),\n",
    "                        df['Pclass'].values.reshape(-1, 1),\n",
    "                        df['SibSp'].values.reshape(-1, 1),\n",
    "                        df['Parch'].values.reshape(-1, 1),\n",
    "                        df['Fare'].values.reshape(-1, 1),\n",
    "                        df['FamilySize'].values.reshape(-1, 1)),\n",
    "                        axis=1)\n",
    "    \n",
    "    # ordinal encoding of Sex, Embarked and Deck\n",
    "    for column in ['Sex', 'Embarked', 'Deck']:\n",
    "        values = encoders[column].transform(df[column].values.reshape(-1, 1))\n",
    "        values = np.array(values).reshape(-1, 1)\n",
    "        X = np.concatenate((X, values), axis=1)\n",
    "    \n",
    "    # the other columns will not be used\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghiles/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X = feature_extraction(data)\n",
    "y = data['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    'mnb': MultinomialNB(),\n",
    "    'gnb': GaussianNB(),\n",
    "    'svm1': SVC(kernel='linear'),\n",
    "    'svm2': SVC(kernel='rbf'),\n",
    "    'svm3': SVC(kernel='sigmoid'),\n",
    "    'mlp1': MLPClassifier(),\n",
    "    'mlp2': MLPClassifier(hidden_layer_sizes=[100, 100]),\n",
    "    'ada': AdaBoostClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(),\n",
    "    'rfc': RandomForestClassifier(),\n",
    "    'gbc': GradientBoostingClassifier(),\n",
    "    'lr': LogisticRegression()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "accuracies = dict()\n",
    "for clf_name in clfs:\n",
    "    clf = clfs[clf_name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracies[clf_name] = clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada': 0.7988826815642458,\n",
       " 'dtc': 0.75418994413407825,\n",
       " 'gbc': 0.83240223463687146,\n",
       " 'gnb': 0.75977653631284914,\n",
       " 'lr': 0.79329608938547491,\n",
       " 'mlp1': 0.67597765363128492,\n",
       " 'mlp2': 0.67597765363128492,\n",
       " 'mnb': 0.65363128491620115,\n",
       " 'rfc': 0.81005586592178769,\n",
       " 'svm1': 0.79329608938547491,\n",
       " 'svm2': 0.70949720670391059,\n",
       " 'svm3': 0.61452513966480449}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform hyperparameter optimization on the gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067370</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.151755</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209922</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.317616</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.036399</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.416790</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.792368</td>\n",
       "      <td>0.797419</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.027122</td>\n",
       "      <td>0.009152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.150466</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.308378</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.026308</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.386430</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.551998</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.070659</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.903945</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.828283</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>142</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.792929</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.840067</td>\n",
       "      <td>0.071010</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.016722</td>\n",
       "      <td>0.025457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.289717</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.556351</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.027243</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.832751</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.040316</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.124296</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>169</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.619529</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.079337</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.696430</td>\n",
       "      <td>0.006386</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.898429</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>93</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.910774</td>\n",
       "      <td>0.112868</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.026237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.367043</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.741955</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.019919</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.152238</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.019364</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.484290</td>\n",
       "      <td>0.005170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616723</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.617845</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.046470</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.376454</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'deviance', '...</td>\n",
       "      <td>88</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.959596</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.055057</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>0.015203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.077073</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.166436</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.012668</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.211286</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.005854</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.317090</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.028809</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.420103</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.794052</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.010370</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.027810</td>\n",
       "      <td>0.013905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.158060</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.011825</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.305515</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.006328</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.427144</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.569665</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>170</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>0.031566</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.862469</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.001</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.001, 'loss': 'exponential'...</td>\n",
       "      <td>159</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.819865</td>\n",
       "      <td>0.044821</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.018027</td>\n",
       "      <td>0.023368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.299793</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.803591</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>71</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.019515</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.013561</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.462079</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>112</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.588843</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.792368</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.039078</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.684536</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.064603</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.894845</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.053683</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.309724</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>149</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.442712</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>167</td>\n",
       "      <td>0.750842</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.021874</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.592090</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.037111</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.730218</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>149</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.052202</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1.028401</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>deviance</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'deviance', 'ma...</td>\n",
       "      <td>153</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.781145</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.071689</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.016109</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.064637</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.822671</td>\n",
       "      <td>0.940516</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.951178</td>\n",
       "      <td>0.841751</td>\n",
       "      <td>0.937710</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.932660</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.133021</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.809203</td>\n",
       "      <td>0.974747</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.981481</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.973064</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.004956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.198323</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.986532</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>142</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.017168</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.265228</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.792368</td>\n",
       "      <td>0.986532</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.411112</td>\n",
       "      <td>0.002708</td>\n",
       "      <td>0.786756</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>160</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.005676</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.180221</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.986532</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>64</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.831650</td>\n",
       "      <td>0.983165</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.020657</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.023489</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.358731</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.047474</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.458349</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.795735</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>112</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.651705</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>153</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.071860</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.007936</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.831275</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>153</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.032660</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.459314</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.053769</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.549520</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.794613</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>118</td>\n",
       "      <td>0.774411</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.036822</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.660073</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.796857</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>105</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.000140</td>\n",
       "      <td>0.009655</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.770260</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>0.793490</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>8</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>124</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.045256</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.022221</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1.044595</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.789001</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>153</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.811448</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.065273</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.396497</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>93</td>\n",
       "      <td>0.771044</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.801347</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.014424</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.022050</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.556985</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.799102</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>93</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.804714</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.726409</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.791246</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>142</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.057482</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.020756</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.890494</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>149</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.821549</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.052342</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.023702</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1.188885</td>\n",
       "      <td>0.005782</td>\n",
       "      <td>0.792368</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.5</td>\n",
       "      <td>exponential</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.5, 'loss': 'exponential', ...</td>\n",
       "      <td>132</td>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.991582</td>\n",
       "      <td>0.824916</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.784512</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.088647</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.024019</td>\n",
       "      <td>0.003174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0         0.067370         0.000973         0.616162          0.616162   \n",
       "1         0.151755         0.001609         0.616162          0.616162   \n",
       "2         0.209922         0.001745         0.616162          0.616162   \n",
       "3         0.317616         0.002353         0.616162          0.616162   \n",
       "4         0.416790         0.002917         0.792368          0.797419   \n",
       "5         0.150466         0.001534         0.616162          0.616162   \n",
       "6         0.308378         0.002165         0.616162          0.616162   \n",
       "7         0.386430         0.002278         0.616162          0.616162   \n",
       "8         0.551998         0.002787         0.616162          0.616162   \n",
       "9         0.903945         0.003983         0.791246          0.828283   \n",
       "10        0.289717         0.001711         0.616162          0.616162   \n",
       "11        0.556351         0.002449         0.616162          0.616162   \n",
       "12        0.832751         0.003514         0.616162          0.616162   \n",
       "13        1.124296         0.004820         0.617284          0.616162   \n",
       "14        1.696430         0.006386         0.799102          0.898429   \n",
       "15        0.367043         0.001880         0.616162          0.616162   \n",
       "16        0.741955         0.002901         0.616162          0.616162   \n",
       "17        1.152238         0.004151         0.616162          0.616162   \n",
       "18        1.484290         0.005170         0.616162          0.616723   \n",
       "19        2.376454         0.007636         0.800224          0.942200   \n",
       "20        0.077073         0.001134         0.616162          0.616162   \n",
       "21        0.166436         0.001541         0.616162          0.616162   \n",
       "22        0.211286         0.001754         0.616162          0.616162   \n",
       "23        0.317090         0.002314         0.616162          0.616162   \n",
       "24        0.420103         0.002756         0.793490          0.794052   \n",
       "25        0.158060         0.001371         0.616162          0.616162   \n",
       "26        0.305515         0.001884         0.616162          0.616162   \n",
       "27        0.427144         0.002201         0.616162          0.616162   \n",
       "28        0.569665         0.002755         0.616162          0.616162   \n",
       "29        0.862469         0.004121         0.787879          0.819865   \n",
       "..             ...              ...              ...               ...   \n",
       "170       0.299793         0.001859         0.803591          0.987093   \n",
       "171       0.462079         0.002362         0.795735          0.987093   \n",
       "172       0.588843         0.003531         0.792368          0.987093   \n",
       "173       0.684536         0.003780         0.793490          0.987093   \n",
       "174       0.894845         0.004920         0.793490          0.987093   \n",
       "175       0.309724         0.001653         0.790123          0.987093   \n",
       "176       0.442712         0.002406         0.781145          0.987093   \n",
       "177       0.592090         0.003123         0.793490          0.987093   \n",
       "178       0.730218         0.003965         0.790123          0.987093   \n",
       "179       1.028401         0.005493         0.789001          0.987093   \n",
       "180       0.064637         0.000859         0.822671          0.940516   \n",
       "181       0.133021         0.001234         0.809203          0.974747   \n",
       "182       0.198323         0.001552         0.791246          0.986532   \n",
       "183       0.265228         0.001880         0.792368          0.986532   \n",
       "184       0.411112         0.002708         0.786756          0.987093   \n",
       "185       0.180221         0.001239         0.804714          0.986532   \n",
       "186       0.358731         0.001846         0.797980          0.987093   \n",
       "187       0.458349         0.002398         0.795735          0.987093   \n",
       "188       0.651705         0.003634         0.789001          0.987093   \n",
       "189       0.831275         0.003911         0.789001          0.987093   \n",
       "190       0.459314         0.001928         0.797980          0.987093   \n",
       "191       0.549520         0.002502         0.794613          0.987093   \n",
       "192       0.660073         0.003066         0.796857          0.987093   \n",
       "193       0.770260         0.003528         0.793490          0.987093   \n",
       "194       1.044595         0.004798         0.789001          0.987093   \n",
       "195       0.396497         0.001975         0.799102          0.987093   \n",
       "196       0.556985         0.002518         0.799102          0.987093   \n",
       "197       0.726409         0.003392         0.791246          0.987093   \n",
       "198       0.890494         0.004222         0.790123          0.987093   \n",
       "199       1.188885         0.005782         0.792368          0.987093   \n",
       "\n",
       "    param_learning_rate   param_loss param_max_depth param_n_estimators  \\\n",
       "0                 0.001     deviance               3                 50   \n",
       "1                 0.001     deviance               3                100   \n",
       "2                 0.001     deviance               3                150   \n",
       "3                 0.001     deviance               3                200   \n",
       "4                 0.001     deviance               3                300   \n",
       "5                 0.001     deviance               5                 50   \n",
       "6                 0.001     deviance               5                100   \n",
       "7                 0.001     deviance               5                150   \n",
       "8                 0.001     deviance               5                200   \n",
       "9                 0.001     deviance               5                300   \n",
       "10                0.001     deviance               8                 50   \n",
       "11                0.001     deviance               8                100   \n",
       "12                0.001     deviance               8                150   \n",
       "13                0.001     deviance               8                200   \n",
       "14                0.001     deviance               8                300   \n",
       "15                0.001     deviance              10                 50   \n",
       "16                0.001     deviance              10                100   \n",
       "17                0.001     deviance              10                150   \n",
       "18                0.001     deviance              10                200   \n",
       "19                0.001     deviance              10                300   \n",
       "20                0.001  exponential               3                 50   \n",
       "21                0.001  exponential               3                100   \n",
       "22                0.001  exponential               3                150   \n",
       "23                0.001  exponential               3                200   \n",
       "24                0.001  exponential               3                300   \n",
       "25                0.001  exponential               5                 50   \n",
       "26                0.001  exponential               5                100   \n",
       "27                0.001  exponential               5                150   \n",
       "28                0.001  exponential               5                200   \n",
       "29                0.001  exponential               5                300   \n",
       "..                  ...          ...             ...                ...   \n",
       "170                 0.5     deviance               8                 50   \n",
       "171                 0.5     deviance               8                100   \n",
       "172                 0.5     deviance               8                150   \n",
       "173                 0.5     deviance               8                200   \n",
       "174                 0.5     deviance               8                300   \n",
       "175                 0.5     deviance              10                 50   \n",
       "176                 0.5     deviance              10                100   \n",
       "177                 0.5     deviance              10                150   \n",
       "178                 0.5     deviance              10                200   \n",
       "179                 0.5     deviance              10                300   \n",
       "180                 0.5  exponential               3                 50   \n",
       "181                 0.5  exponential               3                100   \n",
       "182                 0.5  exponential               3                150   \n",
       "183                 0.5  exponential               3                200   \n",
       "184                 0.5  exponential               3                300   \n",
       "185                 0.5  exponential               5                 50   \n",
       "186                 0.5  exponential               5                100   \n",
       "187                 0.5  exponential               5                150   \n",
       "188                 0.5  exponential               5                200   \n",
       "189                 0.5  exponential               5                300   \n",
       "190                 0.5  exponential               8                 50   \n",
       "191                 0.5  exponential               8                100   \n",
       "192                 0.5  exponential               8                150   \n",
       "193                 0.5  exponential               8                200   \n",
       "194                 0.5  exponential               8                300   \n",
       "195                 0.5  exponential              10                 50   \n",
       "196                 0.5  exponential              10                100   \n",
       "197                 0.5  exponential              10                150   \n",
       "198                 0.5  exponential              10                200   \n",
       "199                 0.5  exponential              10                300   \n",
       "\n",
       "                                                params  rank_test_score  \\\n",
       "0    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "1    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "2    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "3    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "4    {'learning_rate': 0.001, 'loss': 'deviance', '...              132   \n",
       "5    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "6    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "7    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "8    {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "9    {'learning_rate': 0.001, 'loss': 'deviance', '...              142   \n",
       "10   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "11   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "12   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "13   {'learning_rate': 0.001, 'loss': 'deviance', '...              169   \n",
       "14   {'learning_rate': 0.001, 'loss': 'deviance', '...               93   \n",
       "15   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "16   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "17   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "18   {'learning_rate': 0.001, 'loss': 'deviance', '...              170   \n",
       "19   {'learning_rate': 0.001, 'loss': 'deviance', '...               88   \n",
       "20   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "21   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "22   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "23   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "24   {'learning_rate': 0.001, 'loss': 'exponential'...              124   \n",
       "25   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "26   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "27   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "28   {'learning_rate': 0.001, 'loss': 'exponential'...              170   \n",
       "29   {'learning_rate': 0.001, 'loss': 'exponential'...              159   \n",
       "..                                                 ...              ...   \n",
       "170  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...               71   \n",
       "171  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              112   \n",
       "172  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              132   \n",
       "173  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              124   \n",
       "174  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              124   \n",
       "175  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              149   \n",
       "176  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              167   \n",
       "177  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              124   \n",
       "178  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              149   \n",
       "179  {'learning_rate': 0.5, 'loss': 'deviance', 'ma...              153   \n",
       "180  {'learning_rate': 0.5, 'loss': 'exponential', ...                7   \n",
       "181  {'learning_rate': 0.5, 'loss': 'exponential', ...               48   \n",
       "182  {'learning_rate': 0.5, 'loss': 'exponential', ...              142   \n",
       "183  {'learning_rate': 0.5, 'loss': 'exponential', ...              132   \n",
       "184  {'learning_rate': 0.5, 'loss': 'exponential', ...              160   \n",
       "185  {'learning_rate': 0.5, 'loss': 'exponential', ...               64   \n",
       "186  {'learning_rate': 0.5, 'loss': 'exponential', ...              100   \n",
       "187  {'learning_rate': 0.5, 'loss': 'exponential', ...              112   \n",
       "188  {'learning_rate': 0.5, 'loss': 'exponential', ...              153   \n",
       "189  {'learning_rate': 0.5, 'loss': 'exponential', ...              153   \n",
       "190  {'learning_rate': 0.5, 'loss': 'exponential', ...              100   \n",
       "191  {'learning_rate': 0.5, 'loss': 'exponential', ...              118   \n",
       "192  {'learning_rate': 0.5, 'loss': 'exponential', ...              105   \n",
       "193  {'learning_rate': 0.5, 'loss': 'exponential', ...              124   \n",
       "194  {'learning_rate': 0.5, 'loss': 'exponential', ...              153   \n",
       "195  {'learning_rate': 0.5, 'loss': 'exponential', ...               93   \n",
       "196  {'learning_rate': 0.5, 'loss': 'exponential', ...               93   \n",
       "197  {'learning_rate': 0.5, 'loss': 'exponential', ...              142   \n",
       "198  {'learning_rate': 0.5, 'loss': 'exponential', ...              149   \n",
       "199  {'learning_rate': 0.5, 'loss': 'exponential', ...              132   \n",
       "\n",
       "     split0_test_score  split0_train_score  split1_test_score  \\\n",
       "0             0.616162            0.616162           0.616162   \n",
       "1             0.616162            0.616162           0.616162   \n",
       "2             0.616162            0.616162           0.616162   \n",
       "3             0.616162            0.616162           0.616162   \n",
       "4             0.754209            0.803030           0.808081   \n",
       "5             0.616162            0.616162           0.616162   \n",
       "6             0.616162            0.616162           0.616162   \n",
       "7             0.616162            0.616162           0.616162   \n",
       "8             0.616162            0.616162           0.616162   \n",
       "9             0.767677            0.851852           0.801347   \n",
       "10            0.616162            0.616162           0.616162   \n",
       "11            0.616162            0.616162           0.616162   \n",
       "12            0.616162            0.616162           0.616162   \n",
       "13            0.616162            0.616162           0.619529   \n",
       "14            0.791246            0.922559           0.781145   \n",
       "15            0.616162            0.616162           0.616162   \n",
       "16            0.616162            0.616162           0.616162   \n",
       "17            0.616162            0.616162           0.616162   \n",
       "18            0.616162            0.616162           0.616162   \n",
       "19            0.767677            0.959596           0.794613   \n",
       "20            0.616162            0.616162           0.616162   \n",
       "21            0.616162            0.616162           0.616162   \n",
       "22            0.616162            0.616162           0.616162   \n",
       "23            0.616162            0.616162           0.616162   \n",
       "24            0.754209            0.803030           0.811448   \n",
       "25            0.616162            0.616162           0.616162   \n",
       "26            0.616162            0.616162           0.616162   \n",
       "27            0.616162            0.616162           0.616162   \n",
       "28            0.616162            0.616162           0.616162   \n",
       "29            0.764310            0.848485           0.808081   \n",
       "..                 ...                 ...                ...   \n",
       "170           0.784512            0.991582           0.814815   \n",
       "171           0.781145            0.991582           0.814815   \n",
       "172           0.774411            0.991582           0.804714   \n",
       "173           0.781145            0.991582           0.814815   \n",
       "174           0.774411            0.991582           0.808081   \n",
       "175           0.771044            0.991582           0.811448   \n",
       "176           0.750842            0.991582           0.814815   \n",
       "177           0.771044            0.991582           0.818182   \n",
       "178           0.757576            0.991582           0.821549   \n",
       "179           0.774411            0.991582           0.811448   \n",
       "180           0.804714            0.951178           0.841751   \n",
       "181           0.797980            0.981481           0.824916   \n",
       "182           0.767677            0.991582           0.808081   \n",
       "183           0.771044            0.991582           0.804714   \n",
       "184           0.774411            0.991582           0.797980   \n",
       "185           0.774411            0.991582           0.831650   \n",
       "186           0.777778            0.991582           0.814815   \n",
       "187           0.784512            0.991582           0.811448   \n",
       "188           0.777778            0.991582           0.794613   \n",
       "189           0.771044            0.991582           0.804714   \n",
       "190           0.784512            0.991582           0.814815   \n",
       "191           0.774411            0.991582           0.808081   \n",
       "192           0.784512            0.991582           0.808081   \n",
       "193           0.764310            0.991582           0.818182   \n",
       "194           0.771044            0.991582           0.811448   \n",
       "195           0.771044            0.991582           0.824916   \n",
       "196           0.767677            0.991582           0.824916   \n",
       "197           0.767677            0.991582           0.818182   \n",
       "198           0.764310            0.991582           0.821549   \n",
       "199           0.767677            0.991582           0.824916   \n",
       "\n",
       "     split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "0              0.616162           0.616162            0.616162      0.004732   \n",
       "1              0.616162           0.616162            0.616162      0.015602   \n",
       "2              0.616162           0.616162            0.616162      0.005431   \n",
       "3              0.616162           0.616162            0.616162      0.036399   \n",
       "4              0.784512           0.814815            0.804714      0.005550   \n",
       "5              0.616162           0.616162            0.616162      0.008803   \n",
       "6              0.616162           0.616162            0.616162      0.026308   \n",
       "7              0.616162           0.616162            0.616162      0.008046   \n",
       "8              0.616162           0.616162            0.616162      0.070659   \n",
       "9              0.792929           0.804714            0.840067      0.071010   \n",
       "10             0.616162           0.616162            0.616162      0.012874   \n",
       "11             0.616162           0.616162            0.616162      0.027243   \n",
       "12             0.616162           0.616162            0.616162      0.040316   \n",
       "13             0.616162           0.616162            0.616162      0.079337   \n",
       "14             0.861953           0.824916            0.910774      0.112868   \n",
       "15             0.616162           0.616162            0.616162      0.011087   \n",
       "16             0.616162           0.616162            0.616162      0.019919   \n",
       "17             0.616162           0.616162            0.616162      0.019364   \n",
       "18             0.617845           0.616162            0.616162      0.046470   \n",
       "19             0.922559           0.838384            0.944444      0.055057   \n",
       "20             0.616162           0.616162            0.616162      0.003222   \n",
       "21             0.616162           0.616162            0.616162      0.012668   \n",
       "22             0.616162           0.616162            0.616162      0.005854   \n",
       "23             0.616162           0.616162            0.616162      0.028809   \n",
       "24             0.774411           0.814815            0.804714      0.010370   \n",
       "25             0.616162           0.616162            0.616162      0.011825   \n",
       "26             0.616162           0.616162            0.616162      0.006328   \n",
       "27             0.616162           0.616162            0.616162      0.026129   \n",
       "28             0.616162           0.616162            0.616162      0.031566   \n",
       "29             0.791246           0.791246            0.819865      0.044821   \n",
       "..                  ...                ...                 ...           ...   \n",
       "170            0.984848           0.811448            0.984848      0.019515   \n",
       "171            0.984848           0.791246            0.984848      0.012798   \n",
       "172            0.984848           0.797980            0.984848      0.039078   \n",
       "173            0.984848           0.784512            0.984848      0.064603   \n",
       "174            0.984848           0.797980            0.984848      0.053683   \n",
       "175            0.984848           0.787879            0.984848      0.018069   \n",
       "176            0.984848           0.777778            0.984848      0.021874   \n",
       "177            0.984848           0.791246            0.984848      0.037111   \n",
       "178            0.984848           0.791246            0.984848      0.052202   \n",
       "179            0.984848           0.781145            0.984848      0.071689   \n",
       "180            0.937710           0.821549            0.932660      0.000624   \n",
       "181            0.973064           0.804714            0.969697      0.001749   \n",
       "182            0.983165           0.797980            0.984848      0.000447   \n",
       "183            0.983165           0.801347            0.984848      0.000268   \n",
       "184            0.984848           0.787879            0.984848      0.005676   \n",
       "185            0.983165           0.808081            0.984848      0.020657   \n",
       "186            0.984848           0.801347            0.984848      0.047474   \n",
       "187            0.984848           0.791246            0.984848      0.006545   \n",
       "188            0.984848           0.794613            0.984848      0.071860   \n",
       "189            0.984848           0.791246            0.984848      0.032660   \n",
       "190            0.984848           0.794613            0.984848      0.053769   \n",
       "191            0.984848           0.801347            0.984848      0.036822   \n",
       "192            0.984848           0.797980            0.984848      0.056641   \n",
       "193            0.984848           0.797980            0.984848      0.045256   \n",
       "194            0.984848           0.784512            0.984848      0.065273   \n",
       "195            0.984848           0.801347            0.984848      0.014424   \n",
       "196            0.984848           0.804714            0.984848      0.032378   \n",
       "197            0.984848           0.787879            0.984848      0.057482   \n",
       "198            0.984848           0.784512            0.984848      0.052342   \n",
       "199            0.984848           0.784512            0.984848      0.088647   \n",
       "\n",
       "     std_score_time  std_test_score  std_train_score  \n",
       "0          0.000002        0.000000         0.000000  \n",
       "1          0.000207        0.000000         0.000000  \n",
       "2          0.000037        0.000000         0.000000  \n",
       "3          0.000441        0.000000         0.000000  \n",
       "4          0.000177        0.027122         0.009152  \n",
       "5          0.000081        0.000000         0.000000  \n",
       "6          0.000405        0.000000         0.000000  \n",
       "7          0.000088        0.000000         0.000000  \n",
       "8          0.000111        0.000000         0.000000  \n",
       "9          0.000063        0.016722         0.025457  \n",
       "10         0.000265        0.000000         0.000000  \n",
       "11         0.000097        0.000000         0.000000  \n",
       "12         0.000140        0.000000         0.000000  \n",
       "13         0.000363        0.001587         0.000000  \n",
       "14         0.000080        0.018713         0.026237  \n",
       "15         0.000182        0.000000         0.000000  \n",
       "16         0.000006        0.000000         0.000000  \n",
       "17         0.000302        0.000000         0.000000  \n",
       "18         0.000298        0.000000         0.000794  \n",
       "19         0.000635        0.029137         0.015203  \n",
       "20         0.000182        0.000000         0.000000  \n",
       "21         0.000251        0.000000         0.000000  \n",
       "22         0.000158        0.000000         0.000000  \n",
       "23         0.000150        0.000000         0.000000  \n",
       "24         0.000151        0.027810         0.013905  \n",
       "25         0.000137        0.000000         0.000000  \n",
       "26         0.000332        0.000000         0.000000  \n",
       "27         0.000126        0.000000         0.000000  \n",
       "28         0.000044        0.000000         0.000000  \n",
       "29         0.000290        0.018027         0.023368  \n",
       "..              ...             ...              ...  \n",
       "170        0.000252        0.013561         0.003174  \n",
       "171        0.000119        0.014108         0.003174  \n",
       "172        0.000328        0.012992         0.003174  \n",
       "173        0.000204        0.015141         0.003174  \n",
       "174        0.000312        0.014108         0.003174  \n",
       "175        0.000037        0.016571         0.003174  \n",
       "176        0.000118        0.026225         0.003174  \n",
       "177        0.000081        0.019309         0.003174  \n",
       "178        0.000111        0.026129         0.003174  \n",
       "179        0.000103        0.016109         0.003174  \n",
       "180        0.000017        0.015141         0.007816  \n",
       "181        0.000043        0.011446         0.004956  \n",
       "182        0.000038        0.017168         0.003637  \n",
       "183        0.000050        0.015141         0.003637  \n",
       "184        0.000255        0.009655         0.003174  \n",
       "185        0.000179        0.023489         0.003637  \n",
       "186        0.000130        0.015307         0.003174  \n",
       "187        0.000162        0.011446         0.003174  \n",
       "188        0.000457        0.007936         0.003174  \n",
       "189        0.000276        0.013837         0.003174  \n",
       "190        0.000209        0.012598         0.003174  \n",
       "191        0.000416        0.014547         0.003174  \n",
       "192        0.000140        0.009655         0.003174  \n",
       "193        0.000063        0.022221         0.003174  \n",
       "194        0.000182        0.016798         0.003174  \n",
       "195        0.000148        0.022050         0.003174  \n",
       "196        0.000119        0.023702         0.003174  \n",
       "197        0.000492        0.020756         0.003174  \n",
       "198        0.000188        0.023702         0.003174  \n",
       "199        0.000191        0.024019         0.003174  \n",
       "\n",
       "[200 rows x 20 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.5],\n",
    "    'n_estimators': [50, 100, 150, 200, 300],\n",
    "    'max_depth': [3, 5, 8, 10]\n",
    "}\n",
    "gbc = GradientBoostingClassifier()\n",
    "clf = GridSearchCV(gbc, parameters, scoring='accuracy', return_train_score=True)\n",
    "clf.fit(X, y)\n",
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8271604938271605\n",
      "Best params: {'learning_rate': 0.2, 'loss': 'exponential', 'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print('Best score: {0}'.format(clf.best_score_))\n",
    "print('Best params: {0}'.format(clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I'll use gradient boosting as a classification algorithm, with the following hyperparameters:\n",
    "* learning rate: 0.2\n",
    "* loss: exponential\n",
    "* max_depth: 3\n",
    "* n_estimators: 100\n",
    "\n",
    "I'll train the algorithm on the whole provided dataset, then I'll generate the submission file using the provided test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghiles/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.2, loss='exponential', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read and preprocess the whole train dataset\n",
    "data = pd.read_csv('data/train.csv')\n",
    "encoders, column_values = dict(), dict()\n",
    "preprocess(data)\n",
    "\n",
    "# extract the features from the whole training dataset provided\n",
    "X = feature_extraction(data)\n",
    "\n",
    "# format the labels in a sutable way for a classifier\n",
    "y = data['Survived'].values\n",
    "\n",
    "# train the classifier\n",
    "clf = GradientBoostingClassifier(learning_rate=0.2, loss='exponential', max_depth=3, n_estimators=100)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghiles/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# read the test data\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "# we'll save the passenger ids because we need them for the submission file\n",
    "passenger_ids = test_data['PassengerId'].values\n",
    "\n",
    "# preprocess the test data\n",
    "preprocess(test_data, test=True)\n",
    "\n",
    "# extract the features\n",
    "X_test = feature_extraction(test_data)\n",
    "\n",
    "# make the predictions\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the submission in a file\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': passenger_ids,\n",
    "    'Survived': y_pred\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
